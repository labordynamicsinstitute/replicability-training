Unknown Speaker  0:02  
And now you just go to your webs web page.

Unknown Speaker  0:11  
It's fine. I pologize ours

Unknown Speaker  0:15  
will get you recorded. No problem audio. Well, you know what?

Unknown Speaker  0:23  
Okay, so

Lars Vilhuber  0:26  
getting there slowly. So first thing, everything I'm going to be presenting today etc is on a public GitHub site. I'll send that around this email for easy reference, but you can find it easily as get up an account later than amyx Institute. And there's the replicability training, which is what we're going to be going through. There's a couple of links within that. And now I've lost their training. There's a couple of useful links in that There, to the various things that I'm going to be talking about and that we're going to

Unknown Speaker  1:08  
wrap up reference. So

Lars Vilhuber  1:11  
it's a bit of a late start, I'm going to start talking very generally about what it is that we're going to be doing here, why we're doing it, how it sort of intersects with the overall movement in the sciences. And then we're going to get progressively more precise. We're going to give you a look at sort of the hook as to what you're going to be doing primarily within the lab is doing replication reproductions, actually replications of it easier to say that's why we serve you right between these two. I'm going to show you what sort of the prototypical ideal rebei provided by the authors might look like. In practice, they almost never looked like that. And so, task is a bit more challenging. I'm going to show you what sort of template report that we start off with a blank form that guides you through the process of doing so will look like. And we're going to then tease that report apart. So I'm going to go through the various things about what do we actually need? And we're going to expand that by and how can it go wrong? Okay, because most of the time, the world is quite complex in these things, and they go all over the place. I've got a couple of quickly anonymized example, actual reports that we've been working on recently. And in particular, I've got one that shows that was, here's our first report that went back to the authors, they submitted a revision and here's what the revision looks like, which ends up being typically that we've were satisfied. It's not necessarily perfect. We do work in a world that has constraints. So as part of the publication process of the scientific publication process we currently Almost always cookin after the authors have gotten a conditional acceptance. Conditional acceptance means now, people are worried about getting it into print, because they've sort of gotten the satisfaction Yes, sign in. But now I want to see it in print, because only when it's print doesn't matter for tenure and all these other things. So we, we try. We have so far been unsuccessful in the last six months to go for a two week turnaround time for any given article. Part of why we constantly hire new students is that we need enough of you guys to handle the flow of papers coming through. To give you an idea on what the scale is that we're processing right now in the lab for the American Economic Association. So we are getting papers from six journals that the American Economic Association has the primary journal, the AR, or the newer journal AR inquiry, I always get that one wrong and the American economic journals Turn the field journals various domains. We've been doing this publicly officially since middle of July. And the statistics that I have at the top of my head are as of end of November, we processed 150 articles 220 reports that went through so you can see every article goes through at least one revision because so far we have yet to find an article that is perfect on first pass. And we handle that with up through then about 10 students. And we have a backlog because that wasn't enough. When you guys are have joined the team will be up to about 20 students, right. Okay. And we are still getting more journals coming in because we haven't yet covered all of the journals that the association

Unknown Speaker  4:48  
okay. So the,

Lars Vilhuber  4:51  
the typical reproduction exercise in your time working through it will be ideally around 10 to 15 hours of your time spent on it, you can work up to 20 hours in the lab. So that's about two articles per week that you might finish. Okay? And that seems to be working out approximately like that, on average, there are some that are, hey, this one is really easy, done in two hours. There are those that are much longer for various reasons that will go into it, which is okay. So for the we'll be spending a lot of time on that, because that's the core of this. There's going to be lunch back in the in the lounge, and then in the afternoon, we'll do some very specific things will get you access to all the different systems that we use. This is not a very integrated system. So we're cobbling together various things that serve our purposes for what we're doing. Our core job tracking and workflow management system is a JIRA system. So there's issues in there You get assigned an issue you follow through workflows, screens, ask questions, things that need to be filled out. So you need access to that. We try and do best practices ourselves. So we actually track the original submission by the author, we put it into a get system within then track the changes that we have to make to the code and do so in a way that if one person starts it and a second person needs to pick up on it because you went on vacation or you got resigned, or you're not the expert or whatever, that that other person can pick up with the changes ready made in a way that's transparent. Okay. So you need access to that system, which is a different system, you will need to do run the code on some system. We are not expecting you to have all the software on your laptop. So that's where we connect you to sizer, where most of that software will be. There's a few other systems that we use, but primarily we're going to be using sizer eyes. sort of where you log on etc. And if these things are good size etc, are new to you in terms of terms we'll get to that you'll understand.

Unknown Speaker  7:07  
Okay.

Lars Vilhuber  7:11  
We also have certain skills that we will need to impart on you, we will teach you a bit about the command line, you don't have to use the command line. But for many things using the command line on Windows on on Mac on Linux is the common denominator, it works the same on all of them. So that makes our life easier to teach you sometimes also more efficient than having to click through lots of things. One command line, you wrote it up fine. We'll teach you the basics of that how far you want to take that, again, there's up to you. And we're going to help you there are some things that you'll need to install on your laptop part because it's convenient. tools such as a text editor, such as client and all these kinds of things can will teach you through that. When we write the reports, we have a certain pattern use certain basic text files, you're not going to be using Word or something like that to write the reports, we're going to be using markdown to sort of give them a pretty look and things like that. This agenda, for instance, is written in markdown. So again, if you've never seen it, you will get a crash course on it today, and you will quickly comfortable with it. We're going to then briefly diverge into what should an ideal replication package look like. So what is a best practice currently, as it should be taught? In general, it is not in general, it is not fully accepted practice. But this is certainly those who teach it can be known as ideal reproducible practices. You're not going to always see that but in the work that you're doing for us, you should keep that in mind because we want you to apply those even if the author's not right. So and then we're going to spend the rest of the day actually looking through some concrete examples on the How we actually do this. And so on, we have a full day, the day will not complete your training because you will go through two or three examples of test articles that we will have you work on until you get to work on the real articles. So you'll be doing that over the course of the next one to three weeks, as time permits, time, your time as well as ours. And then once we're comfortable that you're comfortable with this process, and there will be a lot of communication back and forth, etc. You'll get assigned some real articles to work on, all the way through. You guys are students you're not just cheap labor. So one of the key things that's important to us a, you're going to take away something that is have used for neuroscientific work here and possibly later on wherever you happen to go. Second, you have midterms, you have finals, you have work to do. We are talking responses to that. The only thing we ask is if you think you're going to go deep dive because you've got three midterms coming up. You have no time to spend on this. That's perfectly fine. Just let us know. Okay, you're here at Cornell to study to work on these things. Okay. And we, we think that is really important. So the key thing from us, you have to let us know in advance, you can't just when you sign an article, it goes into some black hole for three weeks, then you come back. Yeah, I had midterms. That's a No, no, tell us it started with three weeks. I'm not going to be able to work on this and we'll find some deals.

Unknown Speaker  10:33  
Okay.

Lars Vilhuber  10:37  
And I'll get to some of the other things about privacy, your privacy, the author's privacy and stuff like that. Later on, we go through. Okay. So that's sort of the big picture. Let's dive into it. And I'm going to pick up the long version year.

Unknown Speaker  10:57  
I should have pre downloaded this. I'm sorry. What's not good?

Unknown Speaker  11:46  
monitoring let's see if that works.

Lars Vilhuber  11:53  
So this is normally How do I turn this thing off? So this is normally an hour long talk, I'm not going to talk an hour, so I'll skip over some of the things. What we're talking about here is we're part of the scientific discussion about what in some circles is called the reproducibility crisis, you may have encountered that term. The usage of that term has increased in recent years. Whether or not concerns about reproducibility or new what this graph shows is mentions of reproducibility as something of interest, going all the way back to the 70s. So in some disciplines, behavioral psychology, for instance, is one of those examples. There has been a lot of discussion about concerns that findings that are accepted are actually stemming from studies that are themselves not replicable and us that it is of concern when you base further conclusions on things that are not fundamentally sustainable, and so on. There has been a lot of activities about why are we here? What is a reproducible reproducibility problem and how can we address it? Right? So these things are not new. So concerns that have been mentioned are low power you're doing a study that has too few respondents you can't actually say that what the effect is has any relevance or can be detected etc. flexibility and analysis your your pee hacking your you're selecting positive results, zero selecting partial results. showing up in those or selective reporting where negative studies or novel studies are not finding their way into the publication. Lack of replication is the key thing here that things might not actually work the way that the author pretends it works, which may be intentional, which might be fraud, which might be unintentional, and the generic concerned about misuse of statistics that are out there. Now, there's also is going to be some sort of discussion science about the best way to do certain things. So there's a fine line between I disagree about just used and you actually misuse that statistic. But these have been concerns since the 60s and 70s. So this is not necessarily something new, but it's also not something that is resolved. One way to think of it is when we go to some of the parts that we're going to be discussing here, is that take a step back and thinking about how the scholarly discourse actually

Unknown Speaker  14:35  
happened. Okay.

Lars Vilhuber  14:39  
Back in the early 20th, late 19th century, as scholarly publications, go back to the 17th century, but think of something that is much more closer, actual books that are journals that have editors and things like that, that you'll find in the library. If there was something empirical in it, you made an argument based on numbers and some logic that derives from that possibly math, then that would have been written in the article, there was very little that didn't exist if it didn't exist on paper. And if it existed on paper for somebody to use, it wasn't booked on the shelf. Okay, so it was relatively simple. Here's the graph that I mentioned. If you have data you need to provide with the article you printed in if it didn't fit into the actual manuscript, you created fold out pages and things like that. So this is an actual photograph of the 1911 American Economic Review. In order to provide the data that they discussed the article that they've done statistics on, they provided the raw data as a footnote. Now, you don't typically get modern journal articles with a USB drive or a CD drive attached. So part of it is that data became electronic. And in that passage from paper to electronic to very big electronic to other things. It was no longer included with the publication and somewhere in the middle. That we've also forgot how to cite that table of paper that I just showed you. We can cite that because it's in a book that's in the shelf that has a number of volume and issue, all that kind of thing. We know how to cite that the exact page number where it is. In the 60s, you had punch cards, they were possibly very temporary, there was some tape behind or whatever you had to feed it into computer. Part of it was transitory, but that box of punch cards that constitutes the Current Population Survey, I don't know how to cite that box of punch cards that sitting somewhere in a computer lab. Okay, nowadays, we're getting to a world where that is a lot better. So it has become a lot easier to do this practices has improved, etc. But there's sort of this intermediate low, where it was really hard to do these kinds of things. Same goes for math, translated into code, translated into punch cards that are shredded at the end, because you could not keep all those contracts. So those programs exist as a theoretical Possibly temporary translation into actual computer code. We are now in a world where computer code sits out on GitHub sits on your computer, etc. So it is something that can actually be included, but it's typically not. Okay. So, if you compare the two, even though the science was simpler in the 20th century was also complete. You had everything you wanted to have the full logic was developed, the math was there, there was no separate implementation as computer code that was possibly an imperfect translation of the logic or of the math, or the data, etc. Okay. And we've lost some of that completeness. Part of what we're going to be doing is we're going to be checking authors efforts to recover that

Unknown Speaker  17:45  
completeness.

Lars Vilhuber  17:47  
So you sort of seen replication reproducibility as words that I around. I'm going to define them here. I'm going to typically be talking for a very stupid reason about replication. Just is more easily said than reproducibility and reproducing stuff and replicating stuff. It just seems easier to say replication so I'm going to use them interchangeably, but there is a well defined definition that is becoming more acceptable. So on a continuum of things, let's start by reproducibility, computational reproducibility and literature. There is a variety of other names to this narrow replication, purification, verification etc. The most recent definition of this particular set of terms comes from a National Academies report, there was a previous report by the National Science Foundation that define these terms this way and this is sort of coalescing as this is the accepted way to define reproducibility and replicability Okay, there are still other definitions of that in particular, you will probably find it engineering it actually is used in a different way. So, there are definitions and other disciplines where this is different but in the social sciences, this The sort of coalescing wage. So, reproducibility is a very narrow sense. You can reproduce what's in this is where we will be mostly active, right? So it's the same data, same code, same methods in context, what we have in front of us produces what we see in the paper. Okay? Now you can start to generalize part of those conditions, right? The definition of replicability is that you brought in some aspects of it, for instance, you are going to use different code or different software. And you can immediately see here that there's a very fine line or a gray zone between these two definitions. And by using the same computer, some argue that already is fundamentally a replication, not a repressive build. I'm not running with the same system. Am I using a different version of the software a different version of state of MATLAB that I'm using to run this? Is that truly a reproduction or is that already a replication right? Am I reading Implementing the logic written in the paper into new code or translating into C code instead of MATLAB code or state of code. Is that a replication? arguably Yes. Is that more complicated? Definitely. So you could also go out and do new data collect, this was run on a survey, we're going to use the same instrument that was used to collect the survey information, but we're going to go to the same population later, two different populations, different places, etc. Right. And this is what's going on a lot. In the behavioral psychology world, for instance, they have protocols about experiments or interviews that they conducted with people and measure their reactions to those incentives or other things. So let's take that protocol. And it was first run at Harvard. Let's go into lab here at Cornell and run it again with a different student population, or go into different environment completely and run it there. That's already set. replications and generalizability then says, Well, what if we relax many of these things we're trying to distill from economics, psychology, etc, generic behavior. This is not just true for the social sciences, you identified a particular frog population in a particular lake in biology, and they reacted the way to the changes of the seasons or something like that. Does that also apply to a different frog population somewhere else? Where does that apply to them? phoebius creatures all over the place, etc. Is this a fundamental mechanism? Or is there are there limits to its generalizability? So now you've got different data, different methods, etc. You're trying to tease out the same fundamental human behavior using a different way of getting at, right, that's generalizability based on what we've discovered. So there's this range of moving from the very narrow to the much broader scientific understanding. Now there's certain things things that aren't necessarily going to be part of the scientific literature. Right. So generalizability emerges from the scientific consensus potentially or discussion Anyway, after years, decades of research, right? computational reproducibility is something that is

very limiting, but also very at the base of these things, many papers will do aspects themselves of replicability, robustness checks, fundamentally, our replication checks. Thank you. You're relaxing certain assumptions about the data, you're looking only at a part of the data. We're only a certain time span of the data. And looking to see, does our conclusions already hold for this sort of further generalization that we're going to be working at the very left side of that graph, we're going to be testing computational reproducibility, but to some extent, that is the basis for many of the other things that are going on. Okay. So I noticed that there were problems already back in the 60s. Seven years. And we're going to be part of an evolution of improvements to address these issues. Right? So we forgot how to cite data and code and make it available to others or it was difficult to do. So when what happened in started to happen in the late 90s, definitely in the early 2000s, was the journal started to implement replication archives, and data and code availability policies. So Furthermore, after this time period, if you were to publish something that had an empirical component, you were supposed to make the data and possibly the code available to others. And in the absence of better methods by depositing in

Unknown Speaker  23:41  
the oldest

Lars Vilhuber  23:42  
replication archive is that the Journal of Applied econometrics, the oldest packages in there, go back to 1985. They've been doing it continuously. And it's still actually the same person who desperately wants to retire. But it's been going on For a while there has been experimented others, the Journal of money, credit banking and the Journal of Political Economy tried to sell it dropped them again came back and did it again, at the American Economic Association. We have archives that go back to 1999, when it was not just policy, but authors voluntarily provided some of these things. It became policy in 2005. And the journals, American economic journals that we will be working part with were created in 2009. So they have had such a policy in place from the very start. There's many other things that help this transparency, move forward, shared open software, that what you almost take for granted, is a novelty of the early 2000s. Right. It's a reaction to complaints in the literature at the time that computational reproducibility was stymied by the fact that everybody was writing their own C code, Fortran code, COBOL code, whatever running And through various compilers coming to differences that because compilers are not perfectly come up with different answers different implementations, numerical differences arising from randomness in there. And so the fact that you have cheap commonly used statistical software when you have to purchase or free like our is a feature of the 2000 that was not present there before. The fact that you can share many of these components. Those of you who use our know about CRAN, right? Those of you stay to know about this discuss software components that is part of the network, etc. So the fact that I can use a package that you created and go to a repository that we have a common reference, those are innovations of the early 2000s that we now take someone for granted but that are nothing but they're relatively recent. There's been better public use shared data. So public use and share does not necessarily mean the same thing you might be used to going someplace and downloading data say from tip homes or From ICPSR, etc, that's a element of public use data that is made easy to use. ICPSR is in fact, not public use data because you have to be at an institution that is a member of ICPSR. To access most of their data, you are at Cornell, you'll find that you may not be able to access ICPSR data that you might have used for some of your term papers or things like that, once you exit and go into private industry or things like that, because they are no longer members. Right. And so there's shared data is shared among the white population. But otherwise, no further constraints on. There are things that are data that are confidential, their personnel data, their tax data, their business, confidential data that are accessible in large share environments, but you have to go through an application process, you walk into a locker room, you have restrictions on it, but you are not the only one to do that. Right. And those are innovations of late 2000s as well, so that didn't exist in that kind of breath

Unknown Speaker  26:59  
earlier than that.

Lars Vilhuber  27:01  
And in economics, you have probably read many working papers and not just publications that has also been around since the early the mid 1990s. In terms of a way of sharing these papers, actually the first Working Papers go back to the 70s and economics these are now emerging and other disciplines, by archives like archive etc, are late 20 teens implementations of the same idea. It's meant to address for instance, the non reporting the normal reporting, etc. where people have complained we don't see knows we don't see negative results. Because the editors of the journals are sort of preventing this from being published. They don't find it interesting, etc. So what's the solution? Let's not have editors, let's just post the papers insert. Okay. Does it work? That's an open question is this new most domains it is in economics. Okay, and I'm not going to talk too much About pre registration, but you will encounter that as well as sort of one way to address sort of hypothesis hacking. So that's all great. But as it turns out, doesn't always work so well, either. So just asking authors to provide us with their data and their code means they're going to provide something that something may or may not work, okay? that something may not actually be what it's supposed to be. And so there's a there was a realization relatively early on that we need some level of enforcement. And that's not just econ is also present in the political sciences and sociology and biology, etc. I know more about the social sciences, so I see what's happening in some of the other disciplines, but in social sciences, that is different. So the idea of having a data editor or somebody responsible for checking the quality of what is being brought in the same way that we check the quality of the manuscripts now Let's check the supplementary materials as well emerged serve millions 22,000 in the late 20th. So there's a beta for the American Statistical Association software section and they're now expanding to all papers there. There is a process in political sciences and the top journals sort of vet the quality of these various materials that are being brought in. There's also ideas about actually looking at the statistics that are being done articles. So science has some aspects of statistical reviews separate from the content review going on. And I'm the data editor for the American Economic Association. I was appointed in 2017. There is a data editor at the Canadian Journal of Economics. Economic journal, which is the Journal of the Royal Society of economics has appointed the data here. So there are a variety of people in my position in Econ, mostly at the top journals. But this is sort of a very active discussion as it goes on. So I had a very productive lunch working luncheon, for instance, at the American statistical associations, annual meetings with about a dozen other editors and other journals who are all quite interested in applying what we are finding and applying the techniques we have, etc. And so let me speak a bit about that. One thing, this is from a recent paper, that's actually the working paper reference it has it was published in 2019. About acceptance of these methods in, in the scientific population, the population of offers, right? So they asked a bunch of authors in a variety of ways about three things. Do you post your study instruments, the questionnaire is the materials that you use to sort of conduct an experiment and things like that online. Did you post data or code online? Or did you do pre registration

Unknown Speaker  30:51  
and

Lars Vilhuber  30:54  
this is sort of the all domains and economics all these lines are somewhat shifted up for everything, but the premier Registration thing, these are all increasing over time. Okay? So if you go nowadays and asking economics almost everybody has done or things it's reasonable to do so posting data and code. Okay? It seems that it's even still not fully accepted to post all the study instruments online and you will encounter that in the work that we do here in the lab. But people will have run their own survey and we will ask them, please post your question or point to where you can publish your questionnaire as well because that is part of understanding the data. That is still not as accepted. And that's why we see less of that being provided voluntarily as okay. So what are the issues? You will find that just about every econ article and I'll stick my attention right now to those will have some data section that says yeah, we use data from XYZ data downloaded from the Bureau of Economic Analysis paper uses data from the current population service, so We can name these things when they are public nuisance starts to get more complicated when people are talking about Iran the survey and they will have given the survey a name, or they use data from a particular company, and they can't name the company or they can't name where they got it from within the company, etc.

Unknown Speaker  32:16  
So

Lars Vilhuber  32:19  
most data sets are actually quite imperfectly described. So what exactly did you download from the Bureau of Economic Analysis because they've got hundreds of data system to provide and so be a bit more precise about that. Many of these are badly document, right? So if you're downloading it from this discuss agency, there probably is some methodology report somewhere out there. There's probably code book and things like that they're well explained. Readers might want to know where that documentation is that if you were actually the author of this survey, or you collect the data at a company, many of those documentation is basic things what this is a categorical variable that lists type of employees. What does ABC actually mean? The Those kinds of things are not necessarily well documented in practice as we observe it. And very critically, this is about replication. It's about letting others also conduct the analysis. That means you need to know where the data is, and you need to have some certainty that you the data might actually be there when the next person comes around to access the data. And all those things are sometimes hard to define, and very often badly. Okay. archives need to be useful. This has something to do with the with essentially the skill set of researchers on how they program. Okay. When you think back on how you got to where you are currently in your studies, did you take the course in software engineering? Probably not. You might have but most people don't. Right? Do we need something like that? There are many bad programming practices that you counter then you will observe, we will have solutions to many of them that are astonishingly simple in the majority of cases that are not apply. So there's many things that go wrong. What's on the screen is, for instance, the fact that many people hard code a path to their personal laptop into the code that they ultimately post, which makes a very simple level, download the code, press play and run it impossible to do. And this was a complaint in the early 2000s, it is still the number one thing that we need to change our programs. Part of it is intrinsic, because some software it doesn't actually make it easy to abstract away from that. So instead of for instance, it is really hard to not have at least one line that makes that definition and then you can move onwards from that. Okay. The other part and that I mentioned earlier, people have a habit of there's CRAN there's a state a package repositories, etc. But people tend to forget what separate teams they use to conduct the code between It isn't always explicitly defined. It's been hard to ignore and packages like R or Python where you need to explicitly import these things. But it's data or MATLAB, if things are somewhere to be found along some search patterns, the software defines, then it just works. And the fact that you downloaded something five years ago that made your life so much easier. And you forgot to provide as part of the package, which makes life so much more complicated. Anybody who's trying to replicate it gets lost in that path. We're going to test a lot of these things in what we do here. And finally, there's the risk of things just disappear, right? People have been Oh, yes, I'm very open about these things. I've got it posted on my website, you will still encounter that a lot of times about I've got the data. I've got the code. I've got x, y, z on my website. Well, your website is your way of doing something two years from now you're going to reorganize it and that URL that you

Unknown Speaker  35:50  
provide, it won't work anymore. It simply won't be there and a website isn't is an ephemeris thing.

Lars Vilhuber  35:56  
There might be websites that have existed in their current version for 20 years and a half been unchanged, that's fine. But it doesn't guarantee that tomorrow it will still exist. There are again, there are ways to address this. We are going to be somewhat more lenient about that. But typically that implies it's on your website, please give us a copy because we can guarantee it's going to be there for a long time. You can't. Same goes for Dropbox links and other things like that people have their CDs on it, that's fine. People have their code on it, that's not fine, because when they stop paying for Dropbox at some point in time, it will disappear. Okay. So let me skip about that example. So the key thing that we're going to be addressing is remediating the problem that not enough articles are actually simply reproducible. Now, testing this have been multiple papers across the years that have found poor reproducibility or replicability of what's going on. There are many reasons why they found these numbers including the colored bubbles that then are from our prior work before I got appointed as data to we did post publication replication, we downloaded materials from one of the journals in the a stable, looked at the files and try to run them all at Find out with minimal changes, what can we get to run? And depending on how you compute these numbers, do you take into account when data for ethical reasons is not available as a lack of reproducibility? Or do you only count those for all the data is present. We found at the time that there were problems running a good piece of the code got partial replicability for about 86% which is higher than what most people reported. But if we only take that of how to perfectly reproduce everything in the article we got about 40% increase, right. You will find that most of the time that is still true because the first submission That we get typically has problems and would qualify as a partial or incomplete replication. And we are making a difference by giving that common back and having the authors correct that and then and only then post. So hopefully, if somebody ran the experiment that we ran three years ago, in five years, they will find that everything was perfect. But again, there are differences in standards do we really want push button replicability, we have papers that have a very extensive list of manual fixes that need to be made. Is it reproducible? Yes. Is it tedious and onerous? Absolutely. And so there's differences between push button replicability and other things.

Unknown Speaker  38:47  
Okay.

Lars Vilhuber  38:49  
The other problem is accessibility of data. just mentioned that you can get various numbers depending on whether you count that as part of the reproducibility part or not. In the past, if data was not open access, click on it, download it, or you can provide it to the to the paper, you've got what was called an exemption. Now, as it turns out, exemption shouldn't have meant you're going to obfuscate all information about where you got the data from, or describe where, what it is, etc. But in practice, that was a very frequent outcome. If you look at the landscape today, it's not that the access self is broken.

Unknown Speaker  39:30  
It's that

Lars Vilhuber  39:31  
there are good reasons why you might not want to bring data into an archive. And there are good reasons why you want to not simply post things publicly. So for instance, if you are as a historian, you went to the National Archives, you looked at microfiche and things like that. Why would you provide the data into a journal the National Archives are guaranteeing that's going to be there for a very long time. So in that case, the archives won't have the date. And maybe I have to go to the National Archives but I can go to another archives. So one of the key things is the description of access has proven, it has not been vetted. It has not been described, it has not been second guessed, because it's not part of the poor description of the manuscripts that we find. We will be testing a lot of that in the work we're going to do here. Because some of these things are simply not part of what shows up in image. So consider for instance, citation I just said earlier, we can cite a book enters into the references, I said, the book, etc. Well, how do I get that book? Right? It might not be in my library, there might only be one copy somewhere in the United States because it's so old that you can find it. So do I need to then go to the Iowa State Library and and talk to somebody in particular or sign something etc. Now, think of all the kinds of digital data that you can think of as well. If I downloaded data from hippos You can go ahead right now click on it go to register and download the data. Can you give me that? No, no, you can't. Because by their terms of use for most of their data, they say, you have to send the next person to us to get the data and again, right? Is it public use yesterday's Is it redistributable? No, it's not. Okay. So there's all these variations on a theme that we will encounter. And for which, in the first pass, most of the papers that we get will have incomplete information. We will second guess that incomplete information as the authors to complete it. So that there's a complete description of Yes, this is public use data, but I can't give it to you. And all you need to do is register for this is not public use data and there's a registration process, you should expect it to take three weeks and it'll cost you $5,000. Okay, you will find all these various scenarios. So we're going to be leveraging additional things that have emerged over time that make our life So when I said this was a problem that showed up in the early 2000, people came about with the idea of, of, of policies that required folks to, to give the data to the journals. What they were worried about was a persistence longevity. If the journals acquired the data, they could take care of preserving it for a long time. They could ensure access because now it's on the journal website. And you can download it from there. And availabilities anybody who had access to that particular webpage at the journal, possibly conditional on a paywall might be able to

Unknown Speaker  42:31  
download it. Right.

Lars Vilhuber  42:34  
So nowadays, you have archives, digital archives that often called trust repositories and things like that, whose job it is to do exactly that. They're there to ensure longevity of the access and availability to those who are authorized to do these. And so one of the things that has emerged in the past five years really are self deposit repositories that provide this kind of guarantee for really cheap, mostly free, okay, you're doing research, you're collecting data, you want to deposit it somewhere. The ideal is you actually work with a property librarian to generate documentation and to do other things, etc. But at a minimum, you can go to the universe to fix your to organize these are there's a bunch of these various websites and the positive there, they will guarantee that it's there for all eternity, whatever that means. And we'll give you a permanent URL that won't disappear next week called Digital optogenetic.

Okay, and that is also true for the restricted access theosis. So as you increase the level of access, controls competence, person data from data, etc. There are ways that these institutions have handled to sort of ensure that it's available. It's just behind, possibly virtual or physical wall, but it can be described. Now we're still not in an ideal world they often bad They describe their data holdings to the public. But, for instance, the IAB is a German labor, restricted access data environment. And they are starting to do for instance, a great job of also assigning these permanent digital identifies to objects, that you have to go through a full application process to access but they are described, you know where they are, you know, what the conditions are for access. So, while in the work that we will be doing, you will encounter most of the time the researchers coming and bringing the data to us on was one such trusted repository, we do accept that authors have the data on some other trusted repository. It's not very frequent. And we will have to assess is that other place actually a trusted repositories are just website.

So we've done a lot of that. So we are we're in this world of at the American Economic Association, doing these pretty mentors, coaches, we're going to be improving code archives, and you will see that you actually make a difference being that first check that's happening. We can't access all the data that should have been clear by now that we get a lot of data sets that are restricted access. And so you may be working with some data sets where we spend some time actually acquiring the access to them. So currently, for instance, I have one case where we had to apply for access to European business data, I qualify or registered, etc, then took two weeks to finish the email out of my spam, but I ultimately found it. And so now we have access to data to which application. So we are actually now going to run a replication with data that cannot be publicly provided. But we are checking that this is actually true. We have had authors give us a temporary copy of the data saying that I'm allowed to give you a temporary copy, but I'm not allowed to post so please check it out. then delete the data, we have data that will take six months to access or will take $5,000 per year and when people use 25 years of it, so we're probably not going to purchase that access. And that we are just going to do a virtual check of the code etc. We are going to check that, for instance, that statement, it takes six months and $5,000 is true. So we will go to any websites they provide, we might contact any of the emails they provide to verify that that description is fine. But we won't be running some of those audiences. What do we do in those cases where in some cases, we may reach out to parties that have access, we may reach out to somebody within say a research group that is arm's length that wasn't involved in the original research and RA, who is working for your colleague with an institution but also has access to the data can they run with we're going to try and do for the majority of sets. There are services out there that are starting to do this. So for the French restricted access system, there is a service that Behind the French firewall will run the code for anybody and including for us. Okay, so we may be interacting with them and doing some of these things. Most of the time, I take care of those, but you may be seeing part of those analysis because for instance, they'll be checking the computation for reproducibility, we will still be checking the access description, data, citation things, etc. Okay, so that's part of an overall strategy at the journals. Our goal is to create greater transparency with this overall. So the idea is, if I have a data set that legitimately is only usable by one person.

Unknown Speaker  47:36  
Well, you can still generate

Lars Vilhuber  47:38  
valid science based on that, but it is fundamentally not reproducible. Somebody else does something that many hundreds of people can access. They're on the same topic. I'm doing a meta analysis of these various studies, I might want to wait one higher than the other because maybe, theoretically, if not practically, one result is probably more robust than you are more general. Okay, so we want to make more of that information. Because we also want to address the problem that I've heard many times when people talk to me on this topic about, I downloaded this packs and ended up having a bunch of things that aren't runnable because it wasn't clear to me that the data was actually provided. So we want to make this clear. What doesn't a policy like that actually look like?

Unknown Speaker  48:25  
So

Lars Vilhuber  48:27  
I've highlighted here in the someone older policy with the terms actually were right. And this has always been true. As it turns out, it has been perfectly implement that authors needed to clearly and precisely document and make some fashion readily available to other researchers, the materials that support their conclusions. Okay, and those terms clearly and precisely and readily available are ill defined. We're here to test something that used to be that the policy said it has to be prior to publication. So the article is accepted. Now you're running after the author to sort of get them materials and that might affect incentives. And then these were going to be posted on the website.

Unknown Speaker  49:13  
Okay.

Lars Vilhuber  49:16  
So what we've modified in there are clarifying some of these terms more precise, so readily available was interpreted to mean it has to be open access, taking a bit of a step back, because in practice that is not enforceable. And what we're more worried about is that access to the data and code is clearly precisely documented, and ideally, is not exclusive to the offers. So now, if you are the only person who will ever be able to access, that's when you ask for an exemption. But if somebody else could access the data, maybe it takes months, maybe it takes hours to access the data, describe it, and then you're okay. Makes it easier for other students. We are also going to do this prior to final acceptance of the article. Now, we actually have control on the author and incentive for the author to actually provide those materials in a timely fashion and in a reasonable fashion. So until we decide that the archive is good to go, the article does not become accepted and does not move forward can be called the forthcoming article. It's not going to be published until we say it is there. We do not second guess the editors decision about the scientific merits of we are only assessing the technical merits of the replication package. Okay. And we have never rejected a package. We work with the authors collaborative to do that. And they seem generally to be receptive of that. Maybe not surprisingly, since if I say no, they don't get accepted. But since I've never said no, that's not a credible threat. But we've gone back and forth with multiple authors. And at the end of the process, I have been invited to come to their institution and talk about this because they actually found it to be a useful exercise. So there's hope that while we've been doing this for only six months that over time, more and more people will recognize Standards our make our life actually easier, because we will get better packages.

Unknown Speaker  51:05  
And

Lars Vilhuber  51:06  
the other key thing is that information about access to the data needs to be made part of the package. So that is still somewhat vague. But again, it means that a reasonable person must be able to understand what is written about the access to the data. And we will test that. And in particular, we've changed the idea that you just post to website, we've actually moved into one of these trust repositories. There's multiple benefits that come out of that used to be just simple pigs zip file, possibly large, nothing about contents. Now they're sitting on a website, you can explore them, download individual files, look at the review before you download the whole thing, etc. And we've got experts whose job it is to worry about preservation in the long term, thinking about that particular part of it. Finally, the policy now has an element in there that the data to me and the team will assess compliance with its policy. element in that policy is something that we will assess compliance with prior to publication. Okay. Some general principles that surround what we're thinking of, there are, for instance, data citation principles. And you will find, again that this is poorly enacted by most authors at present because they haven't been taught what these things are, and they are relatively recent. So one term you will find is fair data. What does fair mean? It means finable, accessible, interoperable and reusable. And there's various ways those things are defined. It doesn't for instance, necessarily mean that the data is public use, but it needs to be finable need to know in some way that somebody else can find data. It has to be accessible, so there needs to be some mechanism to access the data. interoperable typically is interpreted to me need to know what the data actually needs to be in a format that you have a reasonable hope of understanding. The social sciences that tends to be relatively straightforward, we have a limited variety of how we access data. This is mostly at more of a concern in say, the Health Sciences where you have all sorts of MRI scanner data and tomography data and all sorts of things, which are custom formats where those formats are possibly unique to a particular project. And so there's a lot of worry about, you're creating blobs of things that three years down the road nobody will know what to do with, etc. In economics in trouble, but for us means we're going to be checking is this in a format that a common and our criteria is somewhat idiosyncratic, that you can read with open source software and so most of our data will be in data but data can be well read by our and by Python, etc. So we're not worried about state of going away in the nobody could use these data again, there are known methods right now even if you do not own say that they know MATLAB, somewhat similar, more poorly documented, because MATLAB doesn't have variable names as a variable labels doesn't have values etc. Ideally, we'd love for each data set to also have values explained. As it turns out and small. Most authors aren't coming forward with that. It is extremely onerous degenerate that exposed so we're slacking on those terms for now. But that's what interruptible really means somebody can access the data in a very different way this guy use data, you want to use Python to access the data, that's interoperability, you're able to actually read the data,

Unknown Speaker  54:35  
that's the AI in that

Lars Vilhuber  54:37  
were usable, is a more general term as that you can actually somehow access decrypted on an archive in etc to serve. Okay. Fair principles rely on good metadata about many of these things. So understanding also the context in which data was collected lives, etc is important. And so what our new platform for instance buys us Is that we have information on? Are we trying to collect information, the authors can potentially provide this information about geographic coverage about the time period about who is covered by this data, how was it collected, etc. Now, note, again, that this wasn't there previously, most authors don't actually know what metadata is when you throw it at them. And so this is not something that social scientists are typically trained at. We actually require only the top three items on this list to be there. We strongly encourage authors to provide some of the others but we are not requiring many of the other elements because it's difficult to sort of exactly say what is a required element for some very diverse datasets out there? If I'm running an experiment, I'm running a survey right here in the lot in the audience going to collect the data deposit there. What's the geographic coverage this room in ethical Ithaca, Cornell Coronel of the United States so those terms aren't always clear before Running sort of experiments or small surveys or things like that, right? time periods covered collection periods are two different things. Do they matter? Right, I can definitely define if I run a survey, you're right now what the collection period is, what is the time period covered? Well, that's part of what I'm asking, what were you doing 10 years ago, and we inside coverage is actually 2010, not 2020, or things like that. Those might be relevant, but they're also always not always well defined. I'm just making you play a game to a certain how your reaction is to be economic incentives. What's the time period color, I don't know, doesn't matter if it's not relevant for the survey, because it's a temple. So we don't require these but we strongly encourage them and there's a bit of judgment call as to how we nudge researchers to fill these in. That's not necessarily up to you to come up with that. That's part of what our job is. But if you have ideas about that, when you encounter it, once you've read the articles and things like that, then please feel free to provide that. We have a checklist for someone who's The reproducibility check follows a well established template. We'll talk about that later. And okay, so let me stop that part there.

Unknown Speaker  57:27  
True.

Lars Vilhuber  57:39  
Okay, so that took a bit longer than I expected to take but we've had a bit of a late start, but I think that's the sort of background in which we're acting and how we are interfacing with that entire interface. I should note that what we are doing here is cutting edge at some level. There are very few other associations or societies that are doing this. When you hear about other people doing this, do bring it to my attention when you get asked about it, you can point to this. This is relatively novel. Now, replication itself is not novel. It's doing it at the stage in the publication process. Many classes empirical classes in the various social sciences have empirical exercises, and they often start by a replication exercise, replicate this article and then come up with new ideas based on that, etc. So replication itself is not novel is the fact that we're actually enforcing it prior to publication that is actually okay. So let me then quickly, segue, I've mentioned a couple of times to what we're going to do in the lab and then and then we'll break for a brief period

Unknown Speaker  58:54  
do some other.

Lars Vilhuber  58:56  
So the core of what we're going to be doing here Is the top part of this screen name and conduct pre publication evaluation of the reproducibility and the quality of the supplemental materials that authors provides. Okay, we're going to analyze those materials. And we're going to spend most of the rest of the morning discussing what that actually means. In particular, when a verified data citations, I'm going to go a bit into what is a data citation I'm going to, in order to do that, I'm going to use public websites that we point authors to when they don't get it, right. So we are going to be assessing their compliance with information that they could have found elsewhere about doing we're not going to try and ask them things that they couldn't find or shouldn't have been obvious to them. Most of the time will be pointing them to the website where there are four examples of what data citations should look like.

Unknown Speaker  59:51  
And so

Lars Vilhuber  59:52  
they have no excuse to not provide it, because here's an example. But there are complications and there are

Unknown Speaker  59:57  
solutions to solve those complications.

Lars Vilhuber  1:00:01  
We are going to assess reproducibility by running the actual code on the provided data. Now to do that you need the data. As I just mentioned, sometimes we're going to go out and request the data. Many times we will have data provided by the authors to us. And sometimes we will second guess their ability to actually give us because as it turns out, data that authors download, it may have restrictions on it that prevents them from giving in to us or to others. Okay, and there's a distinction between giving it to us and giving it to others

actually listened to a small side note, you will often find explanations in say the second response to the data editor. When we serve, send them something back, there's a response that comes and says, Okay, here's how we fix these things. Here's how we explain these things. There, they don't have to explain those things necessarily to us. We should explain it to anybody who reads These materials. And so part of it is that we're not trying to get them to satisfy what we state as requirements. We're getting them to satisfy what reasonable readers in the future might want to ask about these things. We're just the first pass of this. Okay. And so once we have all that together,

Unknown Speaker  1:01:21  
we're going to attempt

Lars Vilhuber  1:01:22  
the replication and then assess what comes out. It's a bunch of tables, hopefully, bunch of figures, hopefully. Do we find all the figures, those that we don't find isn't reasonable that they're not those that we do find? Are they the same? Do they appear? We're not going to do some sophisticated point by point verification of the fingers, but we're going to look, do they seem to have the same things right? Sometimes there might be a different trend in there. Sometimes there might be different lines. Sometimes there might be mislabeling you have found all those things. It is your job to compare them. And now some papers off two tables, one finger, we're done. We're going to take one of those as an example. Sometimes there are 35 tables and 20 fingers. That can become a bit onerous. But we have to do all if it becomes too much to do all of them, we can skip things that are in the appendix, because by definition, what's in the appendix, the editors and the referees thought was not as important as what's the main one. So we might not compare the things that are in the appendix, bunch of robustness checks, etc. to ask is that at least the code that we get from the authors be relevant for producing those things. So we might want to run them, but not verify them so that we know that we have functional code. So again, there's always some sort of small thing that varies from the cert very simple standard of Codex tables we're going to run through, they're all the same. We have in the past, and we may very well do for part for sort of small subgroups of Carve out for side projects do some post publication evaluation of reproducibility. We've done it for historical papers in our staple of journals. There's a few research projects that we sort of sketched out that might involve doing it for other journals as well as to what has been posted in the past. How does that align with possible improvements or things like that? So I'm not going to focus on that that used to be a larger part of what we do. Currently, we have more than enough on our plate to just do the people verification, the fundamental checking is not different. We use different systems to do them because one case because everything is public that we download, by definition, most of the results can be public as well versus pre publication where there is a an embargo essentially on when the data becomes public. So the methods are not the same. The ability to influence the author is not the same, of course, when you do post publication on the circle materials, it's What it is, with the occasional asterisk that if we learn that there's data in there that should have been removed, I have worked with authors to revise the materials to sort of go back for that. And we have a research project that we're looking to take into gear where we're going to incentivize authors to improve the materials they provide in the past now that there is a mechanism to do so. That may emerge over the course of the semester. Meredith will probably work with me on setting that up and that may go take possibly a group of students to do some preliminary testing some of those but the key thing is okay. Let's take a brief break now. And then I'll combine sort of the guided walk in these sort of quick glance over the report and one pass, which will then complete before lunch. GSP is at university mono

Unknown Speaker  1:05:07  
you are muted.

Lars Vilhuber  1:05:17  
Okay, well, we're going to take a break. I'm going to stop the recording for now so that we have a smaller file.

Unknown Speaker  1:05:34  
Okay, let's start.

Lars Vilhuber  1:05:37  
So that was the big overview. What kind of information do authors actually have to prepare their materials for us? So let me actually go and show you what they actually see. So among all these journals, they tend to get points. Today at the live data availability policy, I showed you the highlights of that bit earlier. And that policy also has instructions. It tells them in general about where they went. But things, they can put them on the data code repository, which looks like this. There is instructions here on what to do. And when you start something like that, and there's a description of these various things that they need to do. Now, one of the things you will find is that many people don't actually read instructions very well. So there's a user interface thing. This is just a boring list of things. But for instance, this would seem to be an ambiguous there's a button when you actually log into deposit your stuff that's called publish. And it seems to tell you do not publish, you can actually see the data that they provide prior to publication. And yet we get about one case per week where the author actually gets published. So this is information that we point them to when we find in consistencies with their deposit. And so many of these things they do actually do we get information about this, for most cases, not for all cases. And there are other things in here about what they should be doing. You'll notice here that in the data description, we've highlighted information about the data, that's data citation accessibility, about who can access the data and persistence. Again, most of the places actually don't do that. So we have no problem describing that. Here's how their program should be Notice that they're supposed to provide all prerequisites to that. What data would code what programs what software possibly operating system that are prerequisites to doing that, including version numbers, and where appropriate random seats should be fixed. These are all things that have gone wrong in our replication attempts. Okay. We've never been specifically advised of any unusual across the software. That's not unusual. Most authors assume that in their field, at least the software that they have is relative standard starts to get somewhat more tricky when for instance, we have the occasional archives that will compile code against C or Fortran compilers. Those can be costly, they can be very fickle, matters whether or not you've got version 4153 or 4154. Because there might be a particular bug fixed between the two that is particularly relevant For your analysis colleagues, you can wax lyrical about that for very long. And then we define what kind of metadata we want, we want at least a readme file. The README file should be in simple formats. So while some authors might provide a Word file that is not necessarily the robust way of doing it, although there are lots of open source ways now to also read those. We asked them to label every variable. We asked them about code books, again, you will typically find this actually missing. For restricted access data, we specifically say that we're going to verify the information they provide, which also suggests that they should provide information about how to do these things. We have quite a few examples that we end up pointing them to about how to do this. And then there's instructions for some other things. We actually have some additional instruction And that leads to sort of developing or changing things because the ICPSR website, for instance, is evolving a bit as we asked them to modify things to improve the user interface. We want folks to not provide a zip file. And that's a bit of a challenge because the tradition has been to provide us and file and simply upload the zip file. So on ICPSR, it's actually you're actually able to import from a zip file so that it explodes is upon you can actually see its contents.

We get a fair number of authors who do not import, but rather just upload the zip file, in which case it just sits there. That's one again, I think, to note and to have them fix. We give them some examples on how to do these additional metadata.

We asked them to enter the related publication which is incomplete at the time that they're submitted. They do not know which volume or issue or number of, of the particular journal that they're at, will, their article will be in. That's defined by us later on. And there's a few other things. Now this site has additional guidance. And it's one of the sites I've got linked out there. There are various links in here, which are quite useful. There's a template README, there's discussions about licensing of the data on what we do. And so for instance, the template that we're going to be working off is available to the authors to inspect so they know what their test criteria is. Okay. And we have an evolving set of FAQ. A there are those already on the website that sort of are the kinds of questions that we expected users to have they tend to be somewhat more generic with questions such as Do you think that you'll be able to run my code? Yes, we get that quite a bit.

Unknown Speaker  1:12:12  
But

Lars Vilhuber  1:12:15  
there are also some evolving FAQ that are somewhat more specific on a few things that are based on frequently asked questions that we actually got. So things like, well, I'm supposed to cite my data deposit. How do I know the DUI? Well, it turns out there is currently no way of seeing on the open sky or website prior to publication, what the digital object identifier will be, but it can be constructed. And so if you want to figure out what the deal is for that is we've made a little widget that says okay, here it is. Okay. How do you decide your own well If you go back to this, there are other policies that people are subject to, such as sample references. What's the first thing you see on the sample references?

Unknown Speaker  1:13:11  
How to cite the SS?

Lars Vilhuber  1:13:14  
So if you have a question about how do I say that Bureau of Labor Statistics work, your economic analysis or whatever website? Well, we do provide you with an example. And so we point people to the sample references when they don't say the data. Okay. And there are various ways of doing this. This is a data set that has an ICPSR UI that has been there for years. This is a data set that is supplemental data replication data attached to a prior article. And so for these, you would cite it like this. This is a replication data set at a different journal, where it's sitting on their journal website. And so the only thing you can say is actually the article, the UI and not the data. So there's all these subtleties In there are there are lots of other things that we will run into. Yes. And we will get to those. And again, some of these unofficial guidance websites provide help for that. The other one that we have been pulling together and where we are putting things that are common to all economic stuff, and not just to the ADA. So in question about how do I get the UI for the open ICPSR is relatively specific to the adrenals. How do I say data or how do I dress a particular thing is generic to many of the journals, including the economic journal where our friend and Beth Barcelona's and so we have started to compile and are constantly expanding a set of guidances on how to do various things, their best practices strongly suggested things, kinds of information we'd like to have in the code. Things were data and code might be hosted, if not at the journals. So there's a bit of discussion on this landing page, and then a lot more discussion. on specific pages that relate to various aspects of the information that we want, what do we want about data? How do we think about data and code availability statements which expand on servitization? aspect? What do you want in programs and code? What do to make them more robust? Where can you put them? If not at the journals? How do you cite them? And if you want to worry about licensing, we've got a bit of guidance on that as well. All of this information is public. And we point folks to this as one way to sort of explain Well, look, you didn't do it. We're not going to do it for you. But here's guidance on how to do it, send it back, and we'll help you get to that point. Okay. So those are these links here. Again, I'll send you the link to the replicability training, if you haven't found it already. So you can benchmark those. bookmark those. So what should a property README actually look like. So what is the information that we'd love to have? Well, it should start with. And one of the basic things are these basic concepts here. Okay. We are notionally working off of a flow of data from input data to tables and figures in a paper, right? There's a lot of stuff going on between here. But the basic idea is you have some sort of input data. You will want information on that. You have some sort of cleaning programs, that process that data. That might be complicated things. It might be merging various databases, it might be reconfiguring, re categorizing, doing all sorts of things that get the raw data into the format that your table and figure generating programs want them to be. There might be some helpful auxiliary data, right and sometimes this is useful for the authors, writers, and sometimes Part of simply running these programs, and then you throw them away. And ultimately you end up with analysis data. Okay? Sometimes there is no distinction between these two, you've got one suite of programs that goes from the raw data straight to the tables. Okay, that's fine. This is a generic way of describing this. But somewhere along the line, you've got analysis data that you are running through analysis programs to finally generate outputs in the paper. Okay, that's the very basic schema of what we're looking for the ADA, other journals may decide to do things other way. We want all of us, okay, we don't necessarily want the autorai data because if we can regenerate that on the fly, then that's fine if we have all the inputs, but if we don't have all the inputs, but we can't have the ability data that might be an alternate, but ideally, we want everything can want to process through the whole thing. What if we can't have the input data? Well, we still want the clean programs. We still want to be able to assess if we had the data, would we still be able to run this? Okay.

So, data availability statements help us assess where is the input data? Right? And where did you get it from? So even if you're giving us a copy, we're not satisfied with you just giving us a copy. We want to have the path from the original data where you got it from, to where it is now. Okay. Because we have had cases for for instance, they said, I've given you the data from author Joe DiMaggio. And it looks like live. And we've actually gone back and looked at the original data in a publication and it didn't look like that. So how did you get from that to this? That's a clean program, please provide it to us. And do we do this for all the assets we actually go back to the original source and double check that that's impossible, right? We do it in some cases where it's easy to do or it's obvious do we've had data sets have gone into the Polish national library and copy things from paper. They're onto digital files here, we're not going to double check that that's ultimately up to others to check down the road. That's a thesis. That's a master's thesis. That's some other thing, right? People have done thesis like that, that they've gone back and said, This guy coded all these old papers into this, and then ran analysis audit, I've gone back and, hey, there's actually an ambiguous thing. And he made this choice. And if I make this choice, this changes this other thing. And so people actually do that. All we need to know is to ensure that if you're going to copy from that book in the Polish national library, we know where that book is in that book as well. And we have the file that you provided as the copy of that because that is now high value added and other things. So there often is a lot of work behind that. There's some awesome stuff being done. We're not going to redo all of that, because that's not always possible, but we're going to do our feasible stuff and we're somewhere In the back of your mind, I need to finish this thing in two weeks, and I've got this other one that I need to do there as well. Okay. But that's where data availability statements come in. And we provide in this template read me a couple of examples. And so, here's what they do here for public use data. Here's a description of information about where registration is required. We actually ideally want to know what the raw data maps to if you're giving us a copy of that or what it should be called like when you download it and it hasn't been used name where it should be called in your system. So that once we we downloaded reacquire it, etc, that we can slot it in where it needs to go. Right. So instructions as to Okay, go here, download it from there, color this file, stick it into this directory are extremely useful. We always get them confidential data is tricky. So here is one example. An actual example just barely anonymized of data that has a very idiosyncratic access mechanism. Right. Here's an example for Census Bureau data that lists these out. And there's a nice example from Teresa for that has actually 15 of these data sets, all of which you need to request if you're going to put in a proposal at the Census Bureau. But if you request all of them, you're going to get exactly that final sheet. Right? That's wonderful. And that's what you can point people to, to sort of do this. I said that we actually work with repositories, one of the reasons why we work with the OBS PSR repository is because we can see, essentially what you see is what you get the author deposited this way, we're going to look at it and say, yeah, this visually is what once we press publish is what others are going to see and that's okay. Okay.

And that's one of the reasons we want them to create deposit data here because then there's no additional step where they can get things from like uploading the zip file. Now, sometimes we will get papers that are at an earlier stage, in the JIRA system that will introduce to this will be identified by flag. So conditional acceptance means we're really close to accept were the last in between that. But occasionally, we will get papers that are at what's called an r&r, or revise and resubmit. We're only going to get those when the editor things that this paper is likely to be published, there's very little chance of things going wrong, but we're still not quite there. There's still things the author needs to do. But in the meantime, while the author is working on the manuscript to get it into a state where the where the editor can say, conditional acceptance, Hey, why don't you send this stuff over to Lars and his team to check on the other stuff. So those are earlier there are in our and we may not get them in the in the repository we may get for those at Dropbox. So in those cases, it is acceptable in the conditionally accepted is never acceptable to Get stuff provided through Dropbox. And we have also seen complications where the code in open ICPSR is incomplete. And there's a separate Dropbox links to get the complete code. No, we need complete code here. And don't worry if you've got 500 files, we can handle it. Okay, don't worry, if you've got eight gigabytes of files, you can handle that. It probably starts to get a bit complicated around 50 gigabytes, but we can handle all these things. Okay. Ideally, the readme then has a list of datasets that maps the actual physical files into where they come from. Right. So this is an ideal table. We have never gotten a table like that. But it is extremely useful. So part of the tasks that you will need to do is to reconstruct this Taylor. I had to figure out, I have all these data descriptions and I've got all these files. How do they map to each other Now again, we're not going to go completely overboard on that, because it is sometimes a very onerous job. Okay. So heuristically, you can probably figure it out. The other side of that is, if you look at this diagram is, we may only have the analysis data as a single well identified file on there. But you're going to be able to see that analysis file and the programs. In the same way that in the cleaning program, you will see inputs and you will see outputs. So one way, for instance, to look at this is to look at all the inputs, all the outputs from these various files, and then see, do I find them as part of the repository? Again, we've had repositories where we're talking about several dozen files of this type, that can be a tedious. Not every repository isn't a very clean structure. Okay. The next thing that we don't eventually get is the computational requirements. You've had some very good read needs that have sort of identified what the computational requirements is I actually sat down for coffee with one of the authors who sort of said, Yeah, I was the guy who gave you that information, which said, you're going to need Julio, which was the only Julian paper we've ever had since we started doing this process. And you're going to need a compute cluster and run it for about 20,000 computers. Okay. Thanks for letting us know, we're not going to run that. But he also gave us the auxiliary files that are produced by the 20,000 hour run, and that are the input to the subsequent processing, post processing all that day. Right. And so that was all very well explained in Arabian and so really good.

We also said, Remember what was in the data code availability policy, we'd like to know what all the other requirements are. So we need various packages. These are some of the very frequent ones, they change over time. There is no version of that in the state archives contrast to CRAN. So we would actually like to know which versions that was when they were downloaded. Ideally, if the license of the software permits the copy of those so that we are certain that we have exactly the same that you did, because that matters, we've had things break because we use some more recent version of these packages. And, and ideally, we have something like a setup program that actually goes ahead and installs all these requirements or checks that they're there, etc. The ideal setup check. And those coming from software engineering actually know how to do this kind of thing, right? If you have ever compiled open source software that comes with a configure script and the script, they check for requirements are, do I find I need these various libraries? Are they there, I need this data file is it there, etc give you a record at the end. There's these pre pre requirements that are missing you need to fix that before we can run this and then we will end up compiling, right. That is all eminently feasible actually to do in our ns data and MATLAB etc. Is are all the data files I need presence are all the packages I need present. And as it turns out, for instance, to check for packages and data, it's not as easy as simply saying library or whatever in our. But there are ways to check that too. And it's a three line program, we provide an example of such a program encourage authors to incorporate that into their revision if they didn't do in the first place. And and those are really useful. The same goes for any of these other things. Which version of pandas did use in Python, which version of tidy or did you use an R, which version of our interviews because that matters, right? And we may not always actually run with that particular version. But it gives us guidance that if something fails, well, maybe it's because we don't have the same version, something changed there, something broke there. With some effort, we could get back that older version, right. There is a complete archive of every our version, every release of every art package ever released every single minor version. So our actually does a lot of archiving of the version, you can get all of that. It's a lot of hassle to install the non latest version of something, but it's feasible. Typically doesn't matter that much. But it might. packages are less robust. And ultimately, we also want to know what you ran it on and how long it took. Because that's going to be relevant. We get a lot of reviews that say, yeah, and this part of the program might run for a while. is a while three hours because you're not going to sit and wait for the results. You might go off and do something else is a while, weeks, three months, you've had all those, right? Except they didn't say three weeks in three months. They said a while. So sometimes we discovered this after the fact and after three weeks, you might get in contact with the other Hey, this program, how long does it actually run? And those are them suggestions. We don't always insist on having the sort of runtime in the readme when it's trivial amounts to three hours, that's fine. and launch the program, you will find you can launch the program, walk away, do some other things. Right. But if it takes three weeks, I think it's important to know. And the incentive for the author to provide that is that most of that running, and we won't accept your paper until it's done. But if you tell us that it takes three months to run, then we won't wait that time. So that is some of the requirements there. We also want a description of the programs. Ideally, it's a list a manifest of all the program files that are there. Sometimes that can be a bit tedious. So sometimes generic things like a fetish directory that does Data Prep, I've got a directory that does analysis is fine enough if within them then there's a subsidiary description that goes into more detail.

But we want ultimately to be able to associate the program with a table. What do I need to Running to produce this particular table, we also would like actually to have run them all programs out there. So while there needs to be an identifiable sub program that does table 123, ideally, we have something to run them all. Again, there's a huge diversity of ways to run them all. You can have a master script in the same programming language. You can have makefile if you don't know what to make file is welcome to the club. That's about 99% of social scientists, but it's wonderful to run the makefile you might have a single program that just has very well defined sections that say, Here's table one, table two years of data cleaning, etc. It doesn't matter, you're going to find all those as long as a reasonable person can figure out where things are being produced, without having to do all sorts of contortions. And that's fine. We often find that people will write out mnemonic names. So this is table Ola. With controls dot XLS. Okay, that is useful for you while you're developing the paper as an author.

Unknown Speaker  1:31:09  
For us as

Lars Vilhuber  1:31:10  
replicators, it is somewhat more tedious because that Table to Table three. So at a minimum table that maps these mnemonic names to the actual number of tables is, again, something extremely useful, and very few authors provide. This is where the sort of template README comes in as sort of a guidance on how things look, and people are starting to pick up on this. So we've seen a few that have taken our template and and completed it relatively nicely within the need instructions. After all, this prep, now we have are pulling things together, we need instructions, how do we get from empty directory to actually reproduce these tables? Now, ideally, I said we'd like a master runtime. What are the instructions to run it there on the master file should be as easy as that. So Sometimes it gets more complicated. The most complex one we've ever seen is about 32, manual changes spread over pros over eight pages of PDF kind of works, a lot of scope to get things wrong. Are there ways to automate things like that? Absolutely, there are. But we are not here as a consulting company for efficient coding in the social sciences, we're here to check that it actually works. So we will actually do those things that you might be the poor ra gets tasked with trying to figure those eight lines out. And in doing so you find that it's easier to translate those things into a script First, we may actually send the script back to the author and say, Look, this might actually be more efficient, because that's how we ended up doing it. But we get all those things. Okay. And there's sufficient and there's excellent and we're not grading these, but there's this implicit. So ideally, we have something like these, like this table. So again, we're providing authors with an example on how to do this. But we don't always get it. And ideally, I said that at the top, we actually have data citations and references and other things. The best way to think of it is that your read me is a scientific document. The scientific document has references references show up in the list of references. So that is one way of doing it. When I said we actually asked for data citations, and I give you examples, ideally, those are in the manuscript. And in most cases, we will ask that the author add them to the manuscript. But there are cases where that is not the right approach simply because there's so much data processing going on there's there's a data set coming from every county in the United States and it's a different provider and whenever there's an enormous effort in collecting these things, but but person wrote to every one of the 3000 counties in the United States, and got county administrator to send them a file. The dissertation says you're going to cite 3000 in a file that is not going to show up in the manuscript. So there's some practical considerations as well. We've yet to find that examples. But we do have examples where people have written to police departments, and they have gone to every Business Registry in every state. So there are 50 files that are deciding, do we want to have that provenance clearly indicated? Yes. So in the readme, those things should show up. And ideally as data citations, are there workarounds to not having like a huge list of very repetitive kind of things that ultimately are not enormously useful? We encourage authors that provide such high value added data sets, going to pull it to collect all sorts of things that are manually written and PDFs and scanning them in or China or whatever, or going to the various business registries and collecting these data. Yes, there is. It's called actually publishing a data product and we have some very nice examples of authors that have done So that registry of business rented a registry of registries of all the businesses in the United States collected by this team of authors is a website, they have published a paper that describes just their data collection and cleaning process. And now they have something they can cite in our paper, Joseph is one reference, namely their own website.

Not every author has and when they come to us, so there are some that I pull out that might take a bit longer than I encouraged to do that, because it gives higher visibility to their data. But it also makes our life easier because now we have a robust other scientists are attendees. Now, it's not sufficient for them to simply post it on the website and extract from this because that generates the second problem. It's on a website, it's not an archive, etc. So we have small pieces of information I sent to them about how you can go about having a fancy website and yet the data is actually on a repository that's robust. And a few people pick up on it and other people have said, I've done this before. I've done it. like this for 20 years, I'm not changing today can can't win them all. Okay, so this is an ideal remain it might change a bit over time, it's not required. It's part of my tasks over the next couple of months actually, to work with editors and other journals to come up with a common accepted standard that we then might say that require this, because if you do it this way, a you can do it this way, right from the start of the project, and then regardless of what journal you go to, they're gonna say, you're fine, we've got it. Okay. So, as for many things I'll be showing you, these are all evolving. So always go back to the latest when you service. Okay, so this is the ideal. There is not a single example I can point you to that takes all these boxes. Okay. There are a few that take quite a lot of the boxes, but they're not getting public because all this stuff that we've been doing, where we've been giving this constructive feedback to the authors since July, all those are only now starting to So the February AR, I think we'll have the first couple of articles that we've processed through our system. And some of our best examples will take a bit longer to show. Okay, so once we have them, we will start pointing to these good examples and identifying those that are actually excellent for these candidates. For now, it's mostly simple. Okay, so that's the readme. Now we're going to go and score them or apply these things. So we're going to write a report that goes back to the authors to sort of say, We checked all these things. What does it look like? Okay, and this is the actual template that we use. This is what you'll be working on. So there are a lot of instructions in there. And as for people who have a PhD and 10 year and multiple hundred publications as scientific authors, and yet can read instructions. As it turns out, many of your colleagues also are in world. So there's one thing in here, which is right at the very top. All these instructions are meant to be deleted their instructions for you on how to fill up the stock. Okay? But once you've done that these are the key things, everything that's not an instruction is either example code or can be left in there. So for instance, this is generic guidance that we leave in there for every author to look at the unofficial verification guidance, which is exactly the kind of information that we're providing here. Okay. The summary is one of the things that I will fill out or that Meredith will fill out or that some of your team leaders will fill out. And so we then go through the data description. What What should it show, it should show what the data availability and other things that template README should have in there. All of these are elements that are in there. We check the ICPSR they never deposit this if they deposit somewhere else, then It's not the same. But we check that these things are here. And then make recommendations. And some of these might not be relevant for the particular paper that you just read it, we've prefilled it, essentially. So that stuff is in there. We then actually check the data that's provided. And in particular, we check four key things can be read, you have the software to do so if you don't then probably add some some sort of proprietary format that we need to worry about most of the time. That's not the problem. Is it in some sort of archive ready format. So data librarians love to have sort of ASCII CSV files, because they're very generic for everything, but they're also very poor, on additional things like labels and values and stuff like that, or what we call custom formats, but to some extent, some of those are acceptable as well. From our perspective, because, well, they're not necessarily future proof because they might be binary files or things like that. They are readable by a wide set of software out there that have existed or can exist for a while. For instance, data has been around for 20 plus years. You can still read files from data versus data version one with today's software. So some of these things are quite robust. Does the data have variable labels that Adam minimum is something that we need?

Unknown Speaker  1:40:26  
And

Lars Vilhuber  1:40:28  
can we find that in there? And then we have a couple of programs that we run through we will have one that will do this check somewhat automatically for status stuff. We also want to check is there any personally identifying information PII in the data sets? So there's a couple of programs that people have developed in research institutions to serve to a a premium fashion check of is there some Is there a variable called first name there and does it have content? Is there a variable called street address that is in there that might be potentially reveal And we'll identify these. And it's not ultimately not up to us to sort of say, Oh, this is wrong, but rather Oh, this has some things. Let me look at it. Oh, okay, this was just the thing called street name, but it actually has xxx in there for everything. So that's fine if there's ambiguity sup to the offer to resolve, but it's something to highlight in the report that I've been sent back to the offers. So it's a, it's a robustness check. Most of the time it comes up wrong, or false positives or identify things that are not actually sensitive, but it's one simple check to sort of see Is there something going on here? We have had in the past, up cape occasions where there were names of respondents or things like that data file that by accident the authors had left in there, we identified and they corrected it, or good code description again, refer back to the template read me we want to sort of have some open All information about how the code that you're seeing the archive is organized.

Unknown Speaker  1:42:06  
And then

Lars Vilhuber  1:42:08  
we have replication steps computing environment. And ultimately, that means that you've, by this time, run the things, there are findings. Okay? If you have figures, you actually want to compare what the original paper version looks like and what the code generated. And give us an example because this is going back to the author again, so that the author can see what we found, because they might not be finding the same thing. So we want pictures to actually go along with that. The same way that we have numbers that we found in numbers that are in the paper as a juxtaposition that go back to the author. A very tricky thing in text numbers. Authors might have and, you know, the rate fell to 52% of all jobs where the 52% come from Can you find it in the code where it's being generated as it point to table, then we're okay. Is it something that isn't in a table and there needs to be code somewhere or some instructions somewhere worked fine. It came from a previous publication, well, then it needs a citation. Right? If you got it from something that isn't part of your data process. So these can be tricky. And there are things that will escape our attention. Are these parameters, are you saying there's three sections to follow? That number is not relevant for this kind of thing? Is it referencing to something that was said earlier? Is it something that a reader might understand because it's saying that Oh, the interest rate fell to 12%. And there's an RN, a table and ours to convention in this discipline to indicate the interest rate, but it's not clearly enunciate and doesn't say that comes from this table and the 12% is calculated by hand. All these kinds of things make things occasionally complicated for these numbers. Okay, ideally, every number is identified in the code. Somewhere, it says, here's the computation I made to come up with that 12%. And might actually compute 12% in the code rather than the calculator next to your manuscript. Right? So there's like a background book like, preparation us. So certain food if you can, like figure it out. What do you mean? So I guess like, how do you sort of like know that if like, there's like a back of the envelope calculation that they have.

Unknown Speaker  1:44:37  
But they don't make it supposedly like this, right?

Lars Vilhuber  1:44:40  
So if you can figure it out in a reasonable amount of time, and you know, your logic comes to that, then it seems that a reasonable person can find you're the one test case for that. If you can figure it out. Then don't spend too much time on it. It should be On how this is computed, and it should be noted not in the response to the editor Oh, no, you should have calculated this way to be in the readme. Ideally, it's in the code. We will accept that it might be in the really, it's not ideal, but it's okay. Okay. So this is sort of the raw one, let me give you let's go to this guided walk, which I put together. So there's the template report again.

So we have guidance on what we're talking about in terms of data. Okay. So the information we're looking for. I've gone through this now a couple of times is sort of a name of a data set, where does it come from and some characteristics of that okay. This can get complicated, right? If I said earlier, we've got input data, well, that input data might be an extract from a data source at some provider. And maybe I can give you that input data. But I can't really describe other than invert four words how I got that. Or I might not actually know because all I can say is that the data provider sent me a copy of that, because it's a private company or something like that. So sometimes that can be complicated. We actually want to know, in some cases where the physical archive for these things are, is this a data file that actually sits somewhere in restricted access center, so we want to start describing that. Ideally, it has a date, when it was created, if it was published, that's fine. And if I got this file sent to me by somebody else, then maybe I should put down the date. I got it. received it. If I downloaded it from a website, I should probably put down. And that's part of the citation standards. When I got it downloaded, I downloaded from there. Ideally, you have a digital object identifier. If you do, and straightforward if you don't know how to cite the data, then stick it into one of these websites and it'll generate citation for you, you can stick it in there. If you are generating the data, then we've got guidance on how to deposit it that then generates a DUI for you or that you can provide to others. Okay. We have extensive data citation guidance. But it can't be perfect because there's a whole world out there on how to cite these various things. So there's a lot of different references where people have described on how to go through this. So you should, at some point in time go through a few of these. Because they're useful, they tend to be somewhat practical. about these various things, including friends saying simple things that are very generic, but who is the author? year is the year of publication? What's the name of the dataset that might already sometimes be a problem?

Unknown Speaker  1:48:18  
What version number is it?

Lars Vilhuber  1:48:20  
Well, sometimes that's just a date. Sometimes it's a very specific version. Ideally, there's a form in which is provided. That's a question of the actual data style, the data citation style. So that might vary. That Chicago style, which is what the ADA follows, doesn't really like that descriptor to be in there. Sometimes it's put in forms of a data set database or something like that might suit something like that. But location name of producers pretty clear. This is a file from a firm in Chicago, called selling. So this is comes from the statistical agency at this URL or or something else? Right? retrieve from is fine. That means you're getting it from a URL, etc. Right? We actually want, at least in the data availability statement in the readme more than just a generic web page to get it from right. kivus backslash data sets has a ton of datasets, Which one did you actually download may not always be obvious if this title allows you to see on this page, a unique data set, that's fine. If it doesn't, then we need more information. Okay. But you can see that even this guidance is not very clear on those kinds of things. So that this is about data citation, but it's not always clear that you have all the information you need for particular examples. Okay. So this data citation page has various guidances various examples, read through it on your own, see that you understand it and compare with the offers tried to provide when they do provide. In essence, if they do provide something, we're probably satisfied if we think it's reasonable. And if if that URL on the dissertation actually leads to something that is clearly the data set that they also name, then that's good. If it's not exactly the style, that's not your problem, all these articles are going to go through copywriting afterwards. And as long as the copy editors have something to go on, then they can handle it. If they don't, then they're not going to see it as a fault. So that's that additional information there. We also want to have some statistical characteristics and some metadata. Now again, this is judgment call, sometimes the table one in the paper or an appendix table might have those very basic statistical characteristics. Sometimes it has nothing. And so we'd like to have some information about that. And particularly if we can't have access to the data so, so we have been actually trying to ask authors, when they have datasets that they can share with us that at least the online appendix, or just the archive contains some information about the statistical characteristics means minimax might not always be there, because those might be confidential and on themselves, but some sort of description of legal values code book, something like that. Think of it this way, you can access the data, but you want to replicate not reproduce this particular study, what kind of data was the input to this, you need to know some of that. Now, some of that requirement should have come from the referees. But that's not always the case. So you kind of want to know, well, if I were to, so this is confidential us data on employment records of people in New York. Okay? If I were to go to the state of Michigan, what should I ask for? So I need something that looks similar. Because then I could, for instance, replicated this study with a different state, if I can get data that looks similar act similar here. Similarly, to sort of flow into this, and that's the kind of thinking behind that you want to have there. Okay. So again, there's a lot of stuff in there. Data citations don't provide all the information. That's why we have the data availability statement that goes beyond that. So Pew Hispanic, does that require registration or not? Does it require approval by an IRB or not? Is there a cost to it? The data citation doesn't reveal that we want to know Okay, that's why beyond that, we actually Want to stay to access description? We want to know both that the description of the access and the access itself is persistent. When, for instance author sometimes say, okay to get this data, you send an email to Sam smith@company.org.

I don't care about Sam Smith, because he might leave his job tomorrow. I care what what was Sam doing? What was his position? If Sam's no longer there, who should I ask for? Is this the Vice President of financial data? Is this the systems administrator? What's the role of the person at the place that you've got this from? Ideally, it's a website. Ideally, it's an application for but that, of course, doesn't always exist, because not everybody has an application form or robust website to describe this access, because maybe this is only the second time that somebody has ever asked for the data. And so this can be tricky. But again, it's it's The basic question you need to ask is, what if somebody went in two years from now and tried to get the data? It's a judgment call with the right information for that is

Unknown Speaker  1:54:13  
even

Lars Vilhuber  1:54:15  
statistical agencies or survey institutes. These are not necessarily robust URLs, they might redesign their website tomorrow, and it might be different. Right? So we want as much information as can be reasonably expected, but we don't want the authors to invent things that aren't there either. Right? This is, as of today, the place where you request access to restricted PSAT data. As an author, you don't know what the PCT is going to do tomorrow. This is where you got the data. And this is where you would send people to find the data in the first place. Okay. And we want to know some of the access conditions sometimes This is defined as an explicit license. Sometimes it's called the use agreements. Sometimes it's both license and entity use agreements. Sometimes it's ambiguous about what the rules are. Again, we found all these, I occasionally talk to data providers to figure out what are their data use agreements, what are their licenses. And just because observationally their equivalent doesn't mean that they act equivalent or that their intention is equivalent. So just last week, I had conversations with two data providers, both of whom are academics providing data for researchers to use, they're always intended for authors for people to download the data. In both cases, they have a copyright notice at the bottom of the page, and no further information about what to do. So the intent is clear. Use my data, but what can you do with the data afterwards is not specified anywhere. I send emails to both first person said I'm perfectly happy for everybody just redistribute the data inside us. Just go ahead and Do it. The other person said, I absolutely don't want people to redistribute data data for free, they can come back to my site again. Both of those suggests that there should be some greater clarity on the website. But it's not up to the author to force that on the data provider. That's part of the conversations that I have is data. Right? So we get as much as information as we can. But sometimes it's not possible. In other cases, it might be sufficient for the author to provide a PDF of an email correspondence or a PDF of the contract that the sign or a PDF of the Access Request and the sign. So for instance, authors might have access to data through a Freedom of Information Act request. Well, that is one access mechanism. Can you share with us what the request actually looked like? Because I might just recycle it for the next iteration when next year's data becomes available to get exactly the same data back. Maybe You said you've got permission from the copyright holder to distribute data? Well, I don't want your word. And I don't want you to tell me that I want a copy of that permission as part of the archive. Okay, so those are other ways that authors can show that they have the authority to provide the information or how they got the information. That is not a website, etc. But it's part of the archive, have an excellent archive that I saw last week that, for instance, has a file of all the various licenses, that they're aware of the data that they are redistributing. That's awesome, right. And they have pointers to all the data that they aren't redistricting because they couldn't get the list. Because you have to buy the data because the author explicitly did not permit redistribution cetera. It said, We stripped the data out, here's where you get the data. You can worry about it in some fashion. So it's, it's it's a great example. Once it comes public, I'll open to it but until now we don't even very well known sites. Like it bums don't have it make it easy to assess exactly what that looks like. So actually figure out what the terms of use are for items, you have to register them. So then you will see them. So you can point somebody prior to that simply to web pages. Here's the terms of use. If you want to upsell, this is what you're you are going to greet. Ideally, those are public websites that you know, before you agree to them with their.

And in some cases, this gets even more complex. So one example I'd point to, is by someone like Cornell colleagues from several years ago, that got data from insurance companies, and they can't name the insurance. They also can publish the data, but they did get the provider to agree excelente when they assign their data use agreement that the data can be made available in a secure environment to other people for the purposes of reproducing. The results, the paper, and the posted as part of the replication archives of pre approval form. I agree to these terms of conditions, here's where the data can be found. And we as authors already delegate that authority to the data provider to sort of give access under these conditions. An awesome example of having thought through the process right from the start, because this needed to be in the Data Use Agreement they signed before they even started the research. Right? And so most people do not have that kind of agreement in their data use agreement, or if they're running a survey in their informed consent forms. Okay. So these are not always present. If they are, we will want copies of those or pointers to those. And finally, persistence is a tricky thing. Where is there? How long is it going to be there? If for instance, you tell me that the data is that was the other example is say at the Department of Education in Massachusetts. Yes, you can request the data again. Well, how long will they keep a copy of the data? Are they legally required to keep it for some point of time? That may be one indication? Or is the best you can say, well, when we got there, they still have 15 years of data. So probably it will be there around for a while, but no guarantees. That's the best we sometimes get. But will want to know that we don't want that to be personal correspondence with the author. We want that to be part of the reading to sort of say, this is what I heard. Here's what it is. That's the information we acted upon when we accepted this Argo when we did the processing, etc. Okay. And then there's our guidance here has some additional information about if you want to actually go about preparing your data and things like that what you need to worry about. Okay. That's the big picture about that particular data, code documentation. Again, we have a site where we point people to about what is considered to be acceptable code or things like that. First of all, there's a couple of ideas on some references. In particular kentico. Shapiro's is a very focused way of instructing folks in their lab how they want code to be. I don't necessarily agree with all the recommendations in terms of what I would tell my arrays, but it's good guidance to have a look at. There's a more generic one good enough practices in scientific computing, which I suggest everybody read. It's a very short article, but it has some really nice, very generic, not specific to any particular domain guidance on how code should be. Shouldn't be required reading before submitting an article to a journal would make our life a lot easier in many aspects. Okay, so, again, the readme will show up in various cases here, but we'd also like to have a master script sometimes and some of the more modern ways of doing things That we me and the master script might be the same. If you've worked with our which a few of you have, you can think of our markdown. Our markdown generates a very nice markdown that has all the code if you let it have the document in there, which clearly identifies the processing sequence, and generates nice output as well. And you can provide as a readme and as a master script, right, so the our markdown itself would be the master script, clearly identified on one sequence things around. You might have Jupiter notebooks, those start to be here because they have more requirements. They're not as robust or things like that. But you could also have a Python script or a bash script that in and of itself, contains all the instructions is human readable, has tons of comments in it. That can serve as a readme itself. Okay? We don't see those very often, but those are fine as well. We just need something that has all the answers. If you want and hurry, and all the information and address all the programs that are very slapped together into a single document that let's find, most of the time, you will find discrete documents to do these kinds of things. So easy to do, and so rarely done is to have a configuration file that even if you have a sequence of multiple programs that you run through, they all read the same configuration file, which in particular might include

the path or things like that. When we go into the more hands on training, we will show you how to do this in an state and are which are the more frequent ones that require those kind of things so that we can insert config file that then allows us to abstract away from any specific path the authors have done that allow us to collect some system information, etc, as we go through, but it should be strongly encouraged. And again, this is the public website, we point to a variety of ways how to do that in various programming languages. I already mentioned the dependencies. There are also template setup programs. And I just realized I'm not pointing to them here that one could point to as well. Again, once you know it relatively easy to do in our Can you check whether somebody has in their local installation all of these packages and conditionally install them if you want to run through that. Very simple to do. Should you use things like the packrat package or other things in our? Why not? It's one way of doing it should you fix the CRAN copy that is being done at Emraan as sort of an archive snapshot of CRAN at a particular days that I reliably get without much go about the same version of of our and software installed every single time I do that. Also another way of doing it all those are our specific Should I test for and installs data packages There's two or three ways to install a set of packages. So very simple guidance program on how to do that. That one can do yes, you should, should they be installed locally? So that they're part of the project so that they come with the package for us. So we don't actually have to install it. You did when you did the whole thing? Absolutely. So all these are things that can be done. That are very simple best practices. But that mean, most authors don't get to. Okay. things not to do or things that go wrong. And when you see it here, it's because we've actually seen it, right. So this particular example, has some convoluted ways of going through various complicated modifications to the code. Do it this way, run it through, look at the results, change it back changes other thing if you don't change it back might go wrong. Doing this See it? Yes, you can read through this and do it. Not how we want you to do it. One of our favorite bugaboos in Steena is the outbreak package because there are out essed or whatever, because there are multiple versions that depending on how you install it, you'll get different behavior. So unambiguous references to packages in our in state and MATLAB etc, are key to reliable reproducibility. And then I mentioned make as one way to sort of automate things, there are more sophisticated runtime systems. And those sophisticated runtime systems have themselves requirements so often, to get automation, you actually need a lot of other prerequisites to install in so there's a fragility potentially as well, in doing so. So one of the most reproducible labs that I know of provided an archive that was extraordinarily onerous to set up but once it was set up, it ran just fine. Just took ages to set up And so there's there's an arbitrage here as well as about what is what are their overall prerequisites to running things out there. So we pride a couple of things, we'd love to have some packages, I need to replace this graph with a more recent one, which kind of shows that most of what we see out there is data in MATLAB. And so that's why we focus on most of those examples, if you're looking for are somewhere down there.

Okay, so those are again, starting to get a bit more precise about what we're looking for. There. I could spend hours talking about all the things that can go wrong that have gone wrong, because reason I can talk about is because we've seen it, you will encounter novel ways of doing that. We have a few things where we can go about this. So I provided here an annotated Report. It's not an actual report, but things that one can do in the report that you are going to be writing, just for instance, okay, these guys use threat data, you'll learn as an economist with Fred is it's very convenient way to put things. The authors list these things out you do it to the data is provided somewhere and described in the readme. But these things are not cited. And so we add a suggestion that please add data solutions to the article. This is an actual example from slightly paraphrase from whatever ways in this case, we actually have that if you actually go to where they got the data from. Well, lo and behold, inside itself actually has a suggested data citation at the very bottom. So in this case, it is actually really easy for the author to edit his intuition because data provider suggest one, most don't. So it's up to the author to construct it. But a few actually do. And so there really is then no excuse to it. We don't always provide suggest data citations. But when we do go in and check where the data came from, and we see them, we will put it in the readme to make life a bit easier for the authors to comply with it. You might have a second data set that's called the equity index for Mexico. The author's described that they got it from a company, so it has to be bought, it didn't provide it, which is why they took it out of the replication package, but they provide the exact information that if you have purchased access to data from this company called era analytics that you can find it again because it has a unique identifier. But yet, even though that information might be there, you still need to add to the citations. Now it becomes complicated. How do you cite here analytics? Okay, so you go back to the generic data sentation This is up to the author to do. Right? Do you know who's providing the data? Who's the copyright holder? Yeah, era analytics. What is the data set call, both probably on your receipt or on your order or whatever you do that. When was it published? Well, it's when you last accesses kind of the publication date.

Unknown Speaker  2:10:24  
What format is it in the data set?

Lars Vilhuber  2:10:26  
Who's the provider? Well, the author is a random links to providers, also Hager analytics. So that's the publisher and the author. We've just constructed a data citation. Got all the elements there. And we might throw in a link to where Hager analytics describes their data set. Okay. That's it. That's a data citation. It's not hard, but it's absent here. So if you can think about how to do that the author can probably too and that's kind of what's in those sample references Okay. So then that describes all the raw data. Now you might have some analysis data that was provided. In this particular case, the analysis data was called this combined all the raw data sets as part of this, there are no separate raw data. between that, that actually turns out to be the thing that we need to report back to that particular user as to please tell us how you actually compile this because if we actually go into labor analytics and download that particular data set, we need to compile it into this analytics data file. So the analysis data file is the list of the data files that are out. Now, a complete archive will have both the source data and the analysis data. You may not obviously be able to distinguish those again. Some of that help should come from the author and the readme should clearly identify what are these reading through the program sometimes can help as well. So we want that information in there. Well, those are all the data sets. Now we go in and look at what it actually looks like in the icps our

Unknown Speaker  2:12:09  
repository.

Lars Vilhuber  2:12:12  
And so remember, we had a checklist about what the author should provide to us. This is a copy from an actual deposit. So they provided the jail classification, the manuscript number, and phenotype. We didn't provide many of the other elements that we suggest, is it relevant in this particular case to have geographic coverage? is it relevant to have time period? It's not you don't check off the box because I didn't provide it. When I compile the summary based on your replication report, I may suggest specific things as as data and try would like to have it in there because there will write and they're typically suggested materials not required. Simply Because I'm not going to strongly insist on it. But there are arguments to be made that makes your data more finable, etc. I will put words like that into the report to encourage the authors. I tend not to abuse the dictatorial power that I might potentially think I have. So I tread lightly. Do we provide additional guidance on this when they get it wrong? Yeah, you don't want to make your life hard and try to invent it. You missed it the first time around. But here's the things we're asking you to do. Here's the link that walks you through the process of doing that again, people typically get it right on the second pass. Okay. We already talked about the data checks. This is when you start to download the data you've downloaded etc. You're going to look at it, open it, run through it, figure out a few things. This is written because most data even often for MATLAB or our processing will be in state of format, because that's what most economists grow up with. If you find that you've got Arduino or you've got MATLAB data, etc, then all these things might apply as well. Do we have programs to simply run through MATLAB data to check for identifying information? No, we don't, hasn't been an issue so far, mostly not because most of the math has been in macroeconomics or the idea of having person level data or from level data is typically not their business level data might might be. But that doesn't mean that we're not going to encounter say some structural model that is running MATLAB or things like that. So we don't have the best tools to do it there. But if the data is in state of format, then you can run this with data out there. Okay.

These are the actual questions you will see in the template. So these instructions are what you see. Note that we're not asking folks for their variable that I may change this over the course of the next several months to sort of start looking into it. But many of the papers that we see right now, were are from projects that were started years ago. And to have the authors go back sometimes and run the full code book on data that they have somewhere or properly document with all the possible values are that are out there is often too onerous at this time. So part of my strategy has been to sort of announce ahead of time so that when you are now approaching this year in the r&r phase, and you think that you might get through, maybe you want to do some of this prep work now because somewhere in the future, we might start enforcing this right now, you're not enforcing that we have a complete catalog of the possible values that you did. And it sounds so easy. Yeah, you know, M is male and Fs female, it seems obvious, but let's document because sometimes it might be 01 or etc. But if you've got some of the survey data sets out there that are lunch to know, they might have 1000 variables. And you want me now to go through and document all of them when I didn't initially, that's a lot of work. Should you do it? Absolutely. But in the short time period, and maybe not, maybe we're going to waive that for now and, and worry about it later. And so there's a, there's again, a bit of, you can't ask for things that you didn't tell people that they should have is my reasonable approach to them. And will be more stringent on that later on. There's a second aspect that that my philosophy about how you can't ask for things that you didn't tell people that they need to provide necessarily, after the fact also goes to when they come back with revisions. Right? If we asked for something, we're going to check that they satisfied it if we didn't ask for it and we realized, Oh, this is a minor thing we should have asked for, etc. We're not going to Going to go through another round. I might call it the authors and say, Sorry, we forgot to ask for this, could you quickly do that not go through full another round, because another round might take a week or two might delay it again. So there again, there's, there's a bit of, we told you, you need this, if you do all this, you're fine. We discover things in the meantime, that's on us. Or we'll try and find something. If on the other hand, they provide a solution that is not acceptable, that's a different thing. So your code didn't run the first time you fixed it. Now it comes back, but it doesn't yield the same numbers. Well, then it goes back. So you've satisfied one thing, but a new problem has emerged because of the stuff that you provide. And we have sometimes gone through multiple iterations because at each point in time, we learn new things that were not previously obvious. We had one example to sort of talk from from from that experience. They came and said, we can't do you the code. Because not only is the data in a confidential environment, the code is also So in that environment, and I can't give you that. I said, No, that's not right. I know something about that. And I asked the head of that environment, can researchers in your environment asked for the code to exit the environment and be sent to the journals? Absolutely. So revision one was, let's start by having you send us the code. So they did, send us back the code, and the more complete README that identify all the things that they had done in the paper, etc. We have a look at it. There are all these mentions of this is data that we uploaded into the restricted environment. Well, if you upload it, it means it was outside of the restricted environment, which means you can give it to us. Round Two, please give us all that data that you uploaded from the public us because as it turns out, it's only a very small part that is actually confidential. Right. So as we go through, we learn new things that weren't obviously previously and that is Something that is of course subject to go through, right, we ended up having a pretty good archive coming out of it. And, you know, the the author was receptive to these ideas, you wouldn't get published otherwise. But they tend to also think that the ultimate archive is a lot better and better described. When we end up with this process. That's at least what they say. Those that subsequently invite me to give a conference after I've had them go through four rounds actually are proof that they actually appreciate it. So, code description, now have all the data we've checked the data, etc. Code description, ready mentioned a few things we want to know. You're still just looking at the stuff that the authors have provided. And some of this is actually

not yet in the full template because I realized when putting this together earlier this week that I had actually put it in our main template. We actually want to have both cleaning code and analysis code getting this not always obvious how to distinguish the two, sometimes they're the same. But the basic ideas cleaning code is stuff that transforms the raw data files into the analysis data set and analysis code is what transforms analysis data sets into tables and figures. Okay. So, if the readme contains an awesome listing of all these things, you don't have to enumerate them again,

Unknown Speaker  2:20:21  
this is not a test of you. It's, it's what the author can do.

Lars Vilhuber  2:20:26  
Okay? So you might say, here, read me has a list of all the data files and it is complete, we're done. Okay, if it doesn't, then something like this might be useful. You don't need to sort of explain every single program out there, you're trying to give you a summary of what you figured out from the reading and from the programs. You're going to go at this stage ready through and sort of say, Okay, I, the way the programs are structured, the way I look through the programs, I can't figure out where table five is generated. So how can you actually make that state When you need to know that there is a table fight that needs to be generated, right? So implicit in this, and we have a template, an Excel spreadsheet that actually helps you construct this is a listing of all the tables, and all the figures and potentially of all the inline numbers that you find in the manuscript. Right. So implicit in the code descriptions that you're also going to take the manuscript and we don't ever do anything like this without the manuscript. And the online appendix, if there is one, you're going to go through and this list, there are 15 tables as a line for each and five fingers, and there's a line for each. And now we're going to try and find what program is responsible for generating that. And there's the table has, what is the program name and potentially what's the line number and the program name itself. So for instance, you have one master file that does them all. Then you'll have master file listed there and then possible line numbers to sort of identify, this is there. Now ideally, these are in sequence Ideally, the tables and figures are numbered in sequence actually had papers were even that wasn't true. So in the paper, they show up out of sequence, with the numbering to no idea how it came to be. But that's, that has happened. They aren't necessarily produced in sequence. So we have had code that produce tables to five and seven, and then three, six, and eight. Because the logic of the program work better that way can happen, that's where having the line numbers, for instance, comes into play, to sort of see that and that's where having the spreadsheet that sort of says, Okay, I found the code for this, etc. At the end, there is blank lines in there, then we have problem and you can say table five, I couldn't find what code generates this. Okay. And again, we've had all these cases, oh, I accidentally uploaded the file that overrode this other thing that produced table five, let me fix that, etc. It tends to be an oversight. Sometimes, it's because table five is actually not produced by the code looks like they A table or something like that, but it's actually cited from something else. So the hand entered into Excel spreadsheet just dumped into the program, which then suggests that your line will say is not produced by code and in the readme should say, this is actually from there, and the table should then have a citation. This is table five from this other document. And then that's fine. Now we've documented where the numbers come from. They don't come from code, they come from the literature, and that's okay. Figures sometimes can be somewhat tricky, because for instance, say I have a map, or I have a diagram of a theoretical concept economics, there's a there's a budget, there's a difference curve, and it's somehow draw, how was it drawn? Did you hand drawn? Did you use a program? Ideally, we'd actually like to have the program as well, but that's if it's a theoretical concept. That's okay. That's part of manuscript and less of, of data processing. If it comes from a simulation, for instance, we do need that program, if it is generated from data that's being produced by your code, but then you manually import it into Excel and use Excel to generate the graph. We want that Excel spreadsheet, because that's the program in quotes that generated the figures. So we want all the materials that go in there. It's a map was generated by ArcGIS. Send us the shapefiles. Right, potentially, you actually ran this through Q, a GIS, which is an open source program, which has a command line, we'll need the program for that. So to the extent possible, we want to have information. Sometimes they'll say, Yep, here's the shape file and manually generated this ArcGIS. That is acceptable. Okay, it's not ideal, but it's acceptable. But we want most of that, right. We want the shape files. We want the instructions. Together with that. We can probably figure it out. Do we typically pull up maps and regenerate them? We don't. Okay, because that doesn't work. viral occasionally a special skill that few of us have. But for instance, if you're using q GIS and a program to generate the map where you're generating the map and state of war in our code shapefiles will help us do that to generate. I don't think I've seen our Studio Code reasons, even though they have actually pretty good mapping programs in there. So not use that often.

So, these are not always obvious. But it's always good to question where they come from etc. And we have found cases where they said this figure comes from our prior publication. You have the right to put it into the next publication, but you shouldn't say that prior publication, things like that. So your job isn't to related to things you will find the exceptions that actually are not data driven, but that are not well documented. But if you see that this figure says source from so and so your Fine. And your your report consists of the most frequent missingness that we find is a transforming the raw data and analysis of the clean programs, because that didn't used to be required. And appendix. My appendix is long, you probably don't want to go. And so yeah, we do want the code when we might not run it, but we do want and then the code doesn't identify where specific tables, figures etc are created. If you can't figure it out, in a reasonable amount of time, then we should so one thing for instance, we find is that people have these table names. Sometimes we can figure out I've gone through this exercise myself by saying okay, this is being generated by the program. The caption is generated within status so I can figure out in the code with the captain says and figure out in the paper, what were that Captain Okay, that's in table seven, I'll still send back a suggestion, you might want to actually label or map those things because this is tedious and not very robust. But that's feasible. Okay? We spent a lot of time talking about all this up. Until then it's a lot of work. If you actually go through the application archives to get to here, we haven't even run the code yet. Okay. But we get to this point, because this is something we can do for every archive, right? Whether we have all the data or not, all of these steps can be done for them. If you don't have the data, you're not going to run some sort of data checking programs or API checking programs on a you can and that's what your report will say. Whatever data is being provided will be subject to our labels on etc. Even if it's not complete data. programs, we're going to have them always, right, we get to this point, we write a preliminary version of the report. Okay. Why do we want a pretty An original report at this point, you've invested a lot of time. Now come the next steps. And these are steps that are not necessarily up to you to decide whether or not to them. Data is not accessible. Well, how is it accessible? Can we figure that out? Are we going to make the decision to possibly ask permission from the editor? Hey, this data required has an application procedure. I think we can get it because it says it'll only take two weeks do so are you willing to wait that long until we've got the data to then proceed with the replication of the paper? Sometimes we get yes, sometimes we get no, that's my interaction with the editors, I need the information from you in the Premier report whether that's even possible. Right. So that's where a preliminary reports come in. And your job will be to highlight Hey, during our weekly meetings, which will have Hey, I have this thing looks like we could get access and it might might have a look at it. And in the meantime, I'll work on other things until we figure that out. And so there's a first intermediate step here. The second reason is with Darren, I looked at all these And runs this funky software thing. I have know nothing about this. I don't know how to do this. Right? You are not all in all knowing experts. Maybe even we don't know how to do it. And I know how to do a lot of things by definitely don't know how to do everything that will show up and economics journals. It might be software that isn't on sizer that isn't on our Linux computers that we can't download for free that we can't get a trial version for. We've tried all those things. And we don't have it. But maybe somebody on campus might have it. Maybe we can ask somebody maybe we can figure out there is a grad student who that has been working on this. I heard about that. Let's ask him if you can run that part, etc. You got something that's in complicated MATLAB thing. You aren't sure that you can do it will ask somebody who can write. So we can switch around both priorities and allocations of time and things like that at this stage. If we have all that information if you did the job right report all the information is there. And

actually, this is the part where we get to that information, sorry. So the computing environment needs to be defined. And in particular, what are the versions that the author is required to do them and we'll adjust it to have our versions of that as well. So this is an actual example of some of the hardware that we have at our disposal. We'll get into that this afternoon and sitting you up on that. key thing is, if you want to do all this stuff on your laptop, be my guest. Okay. But we are not allowed to actually buy your copy of say you don't have state on your laptop. This requires data we will provide an environment where you can do this requires our but it is adamant that it has to be an old version of our and you don't want to screw up the install on your laptop. That's why we have these other environments. You want to try it on your laptop because you're going to be on a plane and going somewhere etc. And you think this will work. Perfect with me and reproducibility Here means it has to be reproducible in some environment doesn't have to be our designated go to main environment. Okay? But on the other hand, don't try it on your laptop, if it actually says in there that it shouldn't run on a laptop. your laptop is an old netbook, and, you know, it has half a gig of memory, that's probably not going to work. If you can barely little download the data. That's where we have big computers to go to. Okay, so whatever you want to do, you can run this stuff where it's necessary to run it. But up until here, that assessment can always be run on your laptop. Again, you don't have to, but it can always be run on your laptop because it might be more convenient. nice sunny day, you want to sort of work that out in the sun, take your laptop out there, work on your laptop, do whatever, right. And then you file the preliminary report and then we'll take it from there to sort of see are we going to switch to somewhere else to sort of do that and the way that we structure the work again, we'll go through that this afternoon, with remote repository with remotely accessible computing resources gives you the opportunity to do that to switch between these environments. Okay, we are going to suggest to you a couple of software pieces that you should have on your laptop that make working with these reports with making some of the analysis, either critical or highly convenient, so that we are talking the same light. But otherwise, we're going to be using some when it gets to computing. Our main system that we're going to go to are the cyber systems. Okay. And the reason is because either area today to be they're quite big, sometimes they get overloaded. So that's a problem. And there's a lot of second thing is you can connect and disconnect from them. You have something that runs for 10 minutes, you might stay online, you've got something you know, run for three hours, disconnect, do something else, come back after dinner and check that right. So things can stay running on there, even though you shut down your laptop. That's that's key for this Because there is no reason to sort of constrain yourself to what you're my laptop is running by any to run to work need to pause it, whatever. No, it is something that runs longer. Put it on a big computers leave it running there, right? They run Windows, they can get overloaded. They might not always have all the stuff that is needed. Sometimes we need to install things and sizer. We can't install things because we are users not administrators on those systems. So then we have a couple of virtual systems that are set up to be almost like sizer, but on which we actually have admin rights. For those we actually have to pay extra. So we turn them on and off as needed. But for instance, a very frequent phenomenas that will have MATLAB code it uses an auxiliary program called dinaric denari is not installed on cisors. So we run it on our custom systems. Those systems behave the same way that cider does is same login, you will get your same home directory All that stuff just works the same, but it's a system that we have more control. So if you run into something and say, I can't find this doesn't work on sizer, etc, we fall back to that system. And it might include things like status. So we had a recent thing where data was needed to download things from an online API. That package instead of relying on Java, there is no Java on site. So even with state of running, we needed to install stuff, but turn up a bit complicated. We also have access to Linux servers. So sometimes that becomes convenient. It's not a full blown Linux cluster. When you hear the word cluster, that's something else. But we can explore some of these things as well. If people tell us here's a Amazon virtual machine, and you can copy mine at your own cost and do stuff like that. I have yet to encounter that. But if we do, we can and we will. If it's easy supposed to do. So

we have a set of Linux servers. So this is one of them. This single one has 64 cores, we've got access to five or six more of them. This is our largest one which has a full terabyte of RAM so that you can load very big data sets into memory. It's not the newest machine, so it might not be the fastest. But if we reserve it, it's ours to run with. And it's got 64 cores, instead of the three times whatever size or has lots of people on it. When we get this one, it's ours. There's nobody else on it. So sometimes that may be necessary because we require those resources. Sometimes it's useful because as it turns out, the researcher had used a Linux machine and there's things within it that just work better when you stay within the same architecture. That's all for it. I don't expect you guys to be Linux experts. This is one of those things where we can reallocate right done the proneural But I'm also willing to sort of say you want to do a quick learn step and do this or try this out or you've already tinkered around with it, here's a final opportunity to try it out. Willing, if we have the time willing to work with that as well. So if you want to try that out at some point in time, that's perfect as one. We haven't done this in the past, but I found it to be quite useful so far. And so we're going to be adding into our template is to specifically identify the software that is needed. There's also a way to tag the generic version in the JIRA system, which is where it should be after the preliminary report because that allows us to then reallocate things, but the report should have additional information. The README should have information on what version the authors used. We will have the version that we used in part of the report, report going back to the authors. These are at the time that I put this together, the latest version of the Intel compiler is not the latest version that's invented version. And finally, you'll come back to the section once we finished running the code to identify the runtime. There's two reasons why this is useful for us. And we haven't done this in the past. One is that we will get revisions. And they may require us to run through this again. So it might be useful to know how much it took first time around to see how much time it would take the second time. We may not rerun all the code the second time round, if it says this took 20 days, maybe we'll just want to try and run that one table that was there. But maybe the code is not set up to run just one table. And so decisions, right. If it takes 30 minutes, we'll just run through the whole thing. Again, we might only focus on table five that wasn't there in the previous time around 20 days. If we aren't going to run it again, this is useful information to report back to the editorial office disrupts a look. We try to do two week turnaround time but this one won't be too So, either you make the editor making executive quality, we're not going to do this, we're only going to do the preliminary assessment part, or this is how long it takes. Okay. preliminary report, you were pointed back, you're going to stick to it, or you're going to pick up somebody else's issue here and run through it. And now we're actually going to run stuff. And so between that part, and the findings are hours or weeks are running through things, right? You will be assigned, in theory to work for one case at a time. But if you see that this is sort of going off, and it's not coming back in 10 minutes, or there's indications in the readme or in the code or and maybe through your experience, oh, this has a bootstrap. So bootstrap, this is going to run for at least two days. Okay. In the meantime, switch two different issues, start working on that, leave it running on sizer, come back, and so running stuff often be along scenario. Now some of the reasons Have you launching things manually every occasion. And so there is an arbitrage between I'm going to cycle in every day and run something that takes two hours. Now, everything that runs two hours actually takes 24 hours, that might not be the reasonable thing to do. So not always clear what the optimal strategy here is. At the end, hopefully, the code will have produced figures, tables, and other things as well. Now, ideally, it will produce only the figures and tables that our paper we've had had code that produces like 100 tables, you're going to have to figure out which one are the 10 tables that showed up in the paper. Again, that's suggestion back in the report that maybe you want to comment out parts of the code or delete parts of the code that produce stuff that isn't being used in the manuscript. And you're going to compare them.

So you'll find sometimes there's differences is not up to us to judge whether those differences are major or minor. It is up to the author to some extent to make an adjustment to that when we get back we require exact numerical replication, reproducibility, etc. And in general, that's what code does it produces the exact same stuff again, if it doesn't, we need an explanation. Why is there some randomness that you didn't control for? Well set random seeds so that you're actually forcing the pseudo random sequence to start at the same time. Did you even if you do that, if you run code in different sequences, you will get different sequences started so it matters if you set the seed then run one then run to to set the seed run to then run one that will generate a different sequence different outputs etc. So the break reproducibility of the code is important for those. Often they simply haven't set the scene until every random will generate different numbers. Sometimes it's not feasible to set C, then there needs to be an explanation the readme as to what are the differences to be expected and why that's the case, right? figures, you're going to include both the original figure and the generated figure, if there is a difference, if there's no difference, no need to do so. Right. And the same then for the index numbers. Now, at the very end, we want from you to sort of have a summary assessment of how replicable this whole thing was, okay? Did you exactly replicate everything, take the top box, where there's some minor things that something needed to run manually or need to adjust things or etc. So there's that there's issues in there. Anything other than changing the directory might be a minor issue and tick the box with minor issues the details are about so no need to sort of go into a lot of detail about that. Partial is when There's a significant number of things that go wrong. Right? You couldn't find out two or three tables they literally did not produce, but others did. Or some data was confidential and some was not. You could only produce the ones that were not. That's partial replication. And it might stay at partial application. Right? Again, the reasons are important as to why is the depression what produces a code that broke? Or is it data that wasn't available? If the data can't be made available, then we'll stay at partial replication. That's the final outcome. If they could be made available, where the code could be fixed, then there's a revision and we'll move on. So part of that is again part of the summary that I have to make afterwards. So you need to provide me with enough information to make that assessment. Ultimately, because it is my assessment that I have to make. And ultimately, there are some things where most or none of the results are up. Okay. Code breaks prerequisites can be found something breaks in an early stage. etc. We have never found where the code runs perfectly and produces completely different results. So that is a theoretical outcome, but it's not something.

Unknown Speaker  2:43:10  
So if you have like a large like me like 40 entries or something

Unknown Speaker  2:43:14  
and like

Lars Vilhuber  2:43:16  
25 entries are wrong, should you artwork, every single one of those, if it starts to get too much, and there isn't an easy way to do it, for instance, it might be a CSV file that you can run a default or something like that. Then, then what we do is that as part of this, we can attach additional materials. So that one could print off a screenshot of the table in the paper and the table that the code generated. And either we can send it off to them as a zip file because they should obviously be able to interpret what those differences are. And maybe have a highlight in the report. So this report is a markdown so you can't attach a PDF or stuff like that. You can put if it comes out in a format that can be pasted into a text file, and you can paste it in there. But typically those aren't necessarily readable. But I can, if you tell me so attach a PDF, because we don't send back the markdown, we send a PDF generated from the markdown, so I can attach a PDF to that. Or we can upload a zip file. So there are ways to communicate more complex failures to the author. Typically, it's also just sufficient to sort of say, look, half the table didn't actually replicate. But sometimes it's useful to do that against it. It's a bit of a judgment call. If you made the judgment call, I don't think this was necessary. I read in this I see a way of doing it. I'll communicate back with you and have questions and can you sort of generate whatever and we'll figure that out. highlight that in the report and or in the final report, and we'll get from there. So we have sent back to authors. Say we made a couple of changes and those over the changes that we requested from the author, we found More expedient to just show him what those changes were rather than giving him some generic blah, blah. But what was needed to change it. Same way that we've sent back PDFs of serve, here's 10 tables and here's every table has some discrepancy. I'm not going to enumerate them, but here's the copy of them, have a look at them and figure out what's going on. So, there are a variety of ways we can respond to these things. Okay, I'm running a bit behind I think for now, we've gone through the complete report. That's what all the things are that needs to be done that need to be reported to be served tabulated here. You are going to have a boatload of questions about all these things as you encounter them. We're going to take a lunch break, there should be sandwiches, including some vegetarian options in in the lounge again. So we'll go back there will stay there probably and this is a note to our Barcelona colleagues. A bit longer will, should come back. around one so we'll start some of the basic things of setting up our lab here that aren't necessarily relevant to the folks there. will come back around one 115 to start talking about command line and stuff like that. And if you guys are still awake in Barcelona, we're still not need to go home then you can join us back I'd say around one for that

Unknown Speaker  2:46:28  
work.

Lars Vilhuber  2:46:34  
You do need to unmute yourself, you're going to say yes or no.

Sometimes Thanks. Good morning.

Transcribed by https://otter.ai
